

\section{Results} 
\label{results} 
This section summarizes the key findings of the experiments that were conducted based on the architectures presented in the sections \ref{ensemExp},  \ref{mainExperiments} and \ref{cont_learning_section} of the thesis. Additionally, this section briefly introduces the simplistic UI. 

\subsection{Cross-domain adaptation results}

In order to achieve fair comparison, all experiments obey the following specifications:
\todo{modify if equipment images are obtained} 
\begin{enumerate}
\item The results presented in the subsequent sections will be evaluated on the T-LESS dataset as introduced in the \nameref{datasets} section. 
\item Thirty objects in the dataset were given arbitrary class names  \texttt{Model 1..Model 30}
\item According to the setup defined in Figure \ref{mymodel}, the \underline{teacher model} accepts only the weakly-augmented images from the \underline{target} domain. The primary objective of the model is to improve performance on the target images. Therefore, the primary objective for this ensembled model is to improve the \texttt{AP} on the teacher model. Thus, \texttt{AP50} results from the evaluation using the target images are used for comparison. 
\item The base learning rate is defined as \texttt{BASE\textunderscore LR} = 0.001.
\item The number of iterations is fixed at \texttt{MAX\textunderscore ITER} = 50 000.
\item The evaluation happens every \texttt{EVAL\textunderscore PERIOD} = 1000 iterations. Considering that there are 50 000 iterations total, that results in 50 evaluation points across the \texttt{AP50} plot.
\end{enumerate}  

\subsubsection{Scheduler adjustment}
\label{scheduler_section} 
First set of experiments was conducted in accordance with the \nameref{ensemExp} section. The original learning rate scheduler was used. The pattern of the original scheduler was presented earlier in Figure \ref{annealing} (a). For the  baseline comparison, the Adaptive Teacher network was initially trained as it is. The results are presented in Figure \ref{original_experiment}. The first graph illustrates the evaluation \texttt{AP50} results for both the teacher and the student models. Although the training process seems idle for the first 20 000 iterations, the model eventually finds better gradients and stabilizes with $max$ (\texttt{AP50}) = 71.39\% at 31 000 iterations. Even though it might seem that the loss value is decreasing by the end of the training, the \texttt{AP50} value also steadily drops. In order to address the noisy predictions shown earlier in Figure \ref{adapt_experiment1}, the learning rate scheduler has been modified to implement a cosine annealing algorithm without restarts \cite{Loshchilov2016}, as shown in Figure \ref{annealing} (b). The original warm-up period of \texttt{WARMUP\textunderscore ITERS} = 1000 is preserved. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./loss&AP50_original.jpg}
	\end{center}
	\caption{Results of the original Adaptive Teacher model evaluated on the custom T-LESS dataset without any modifications.}
	\begin{center}
		\label{original_experiment}
	\end{center}
\end{figure}
Theoretically, this setup would have allowed find better gradients and reduce overshooting. The results achieved after modifying the scheduler are shown in Figure \ref{comparison_1}. Although the \texttt{AP50} value peaked much faster with the new scheduler, it was only able to reach $max$ (\texttt{AP50}) = 62.49\%, which is almost 10\% lower than the original result.   

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_scheduler.jpg}
	\end{center}
	\caption{a)\texttt{AP50} results of the original model and b) \texttt{AP50} results of the model with a cosine scheduler.}
	\begin{center}
		\label{comparison_1}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{Additional augmentations}
\label{augmentations_section} 
Following the ideas proposed in Figure \ref{newAugmentations}, an identical model was trained with two additional strong augmentations. This in theory allows to further diversify the dataset, which will make it less prone to over-fitting. Furthermore, in this and the subsequent  experiments, the model leverages the early stopping algorithm with the $\texttt{PATIENCE} = 10$. The early stopping algorithm will prevent the model from unnecessary training in case if the AP50 does not improve for more than $\texttt{PATIENCE} \times \texttt{EVAL\textunderscore PERIOD} = 10 000$ iterations. The results of the evaluation process with the additional augmentations can be found in Figure \ref{augmentation_experiment}. 
 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_augmentation.jpg}
	\end{center}
	\caption{a)\texttt{AP50} of the model with a cosine scheduler only and b) \texttt{AP50} results of the model with a cosine scheduler and two additional strong augmentations.}
	\begin{center}
		\label{augmentation_experiment}
	\end{center}
\end{figure}

By analyzing these results, it can be easily noticed that the model reaches the peak much faster and the maximum value (\texttt{AP50} = 70.53 \%) is higher than the equivalent model without additional augmentations (\texttt{AP50} = 62.49 \%). Additionally, this model achieves results, which are competitive to the original implementation (71.40 \%).

\FloatBarrier  

\subsubsection{Instance-level DA and consistency regularization}
The following set of experiments evaluates the custom model presented in Figure \ref{mymodel}. The model utilizes the same principles as in the previous experiment, which includes the cosine scheduler and additional augmentations. On top of it, in this experiment, an instance-level domain adaptation is added along with the consistency regularization term. The regularization weights from Equation \ref{total_loss} are initialized as $\lambda_{\text {consist }} = \lambda_{\text {ins }} = 0.07$. Figure \ref{myModel_experiment} presents the comparison between this model and the model presented in Section \ref{augmentations_section}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_Augm_consistency.jpg}
	\end{center}
	\caption{a)\texttt{AP50} of the model with a cosine  scheduler and two extra augmentations against b) \texttt{AP50} of the model with a cosine scheduler, two additional augmentations and a consistency regularization.}
	\begin{center}
		\label{myModel_experiment}
	\end{center}
\end{figure}

As it can be concluded, the plot with consistency regularization follows a similar pattern as the original model with custom augmentations. However, the results drop from $max$ (\texttt{AP50}) = 70.53\% back to  $max$ (\texttt{AP50}) = 62.87\%. In order to verify the design of the custom components and that they work as intended, an additional experiment was conducted. 

Figure \ref{myModel_constloss_total} illustrates the plots of the instance-level loss and the  consistency loss. As it was defined earlier in the \nameref{mainExperiments} section, the proposed model, similarly as any other adversarial DA model, aims to maximize the instance-level alignment loss in order to confuse the classifier and produce domain invariant features. On the other hand, the model also aims to minimize the consistency loss to force both instance- and image-level classifiers to generate identical outputs for the same image. Although both terms are working as designed, which can be concluded from Figure \ref{myModel_constloss_total}, the \texttt{AP50} value does not improve significantly compared to the original Adaptive Teacher model. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./consistency_loss.jpg}
	\end{center}
	\caption{a) The total loss calculation is independent from consistency loss and the instance-level loss terms b) The total loss is proportional to the consistency loss and the instance-level loss terms.}
	\begin{center}
		\label{myModel_constloss_total}
	\end{center}
\end{figure}

A small-scale set of experiments has been additionally carried out in order to identify the optimal weights $\lambda_{\text {consist }}$ and $\lambda_{\text {ins }}$. In order to facilitate the training speed, while preserving fairness in the  comparison process, the maximum number of iterations in all experiments was set as \texttt{MAX\textunderscore ITER} = 30 000. Additionally, to decrease the training speed, only the classes 1 to 20 were used for training. The results of six different experiments are shown in Figure \ref{myModel_varying_params}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_varying_lambda.jpg}
	\end{center}
	\caption{\texttt{AP50} values for varying weight parameters of $\lambda_{\text {consist }}$, $\lambda_{\text {ins }}$ and \texttt{BASE\textunderscore LR}.}
	\begin{center}
		\label{myModel_varying_params}
	\end{center}
\end{figure}
\FloatBarrier  

From these results, the plot with the $\lambda_{\text {consist }} = 0.07$, $\lambda_{\text {ins }} = 0.07$ and \texttt{BASE\textunderscore LR} = 0.001 were identified to be the best parameters with \texttt{AP50} = 73.26 \%. However, in practice, more comprehensive experiments should be carried out to determine the best trade-off parameters and the learning rate. 

In order to verify the performance of the components, one last experiment evaluates the network without the cosine scheduler, which seemed to negatively affect the performance of the subsequent experiments the most (see Figure \ref{comparison_1}). The results of the Regularized Cross-Domain Adaptive Teacher model with the original scheduler are presented in Figure \ref{myModel_withOrigSched}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_myModel_origScheduler.jpg}
	\end{center}
	\caption{a) The custom model with the original scheduler b) The custom model with the cosine scheduler c) Original model without any modifications.}
	\begin{center}
		\label{myModel_withOrigSched}
	\end{center}
\end{figure}
\FloatBarrier

According to the results from Figure \ref{myModel_withOrigSched}, the model with consistency regularization, custom augmentations and the default scheduler shows competitive results (\texttt{AP50} = 69.57 \%) compared to the original Adaptive Teacher model (\texttt{AP50} = 70.56 \%, while also peaking significantly earlier (>15 000 iterations faster).  

An additional step has been carried out to verify whether the higher distribution of certain classes affects the model performance (see Figure \ref{tless_distribution_real}). The results of the custom Adaptive teacher model with the original scheduler were filtered out and the average of the first four classes \texttt{Model 1..Model 4} was  compared to the average of the remaining classes \texttt{Model 5..Model 30}. The results can be found in Figure \ref{myModel_withOrigSched_grouped}. 
    
\begin{figure}[htb]
	\begin{center}
	\includegraphics[width=14cm]{./AP50_per_class_group.jpg}
	\end{center}
	\caption{a) Average values of \texttt{AP50} for the classes 1 to 4 b) Average values of \texttt{AP50} for the classes 5 to 30 extracted from the same model results. }
	\begin{center}
	\label{myModel_withOrigSched_grouped}
	\end{center}
\end{figure}

The results of \texttt{AP50} = 53.65 \% for the classes \texttt{Model 1..Model 4} seem to be significantly lower than \texttt{AP50} = 71.32 \% for the remaining classes. However, due to a low number of samples, these results might also suggest that the first classes \texttt{Model 1..Model 4} are simply harder to detect. Therefore, more experiments are needed with higher variety in the dataset.  
\todo{the evaluation on the real pump will be added here if time is sufficient} 
\subsection{Continual learning results}
\label{cont_learning_results} 
\FloatBarrier 
As was introduced in Figure \ref{tless_distribution_rend}, the entire T-LESS dataset contains 30 different models with objects named arbitrary as \texttt{Model 1..Model 30}. To evaluate the methodology presented in the \nameref{cont_learning_section} section, the following procedure was applied:
 
\begin{enumerate}
\item In the initial experiment, the network was trained on classes \texttt{Model 1..Model 30}.
\item The second network was only trained on the class \texttt{Model $N$}, where $N$ is a number between 21 and 30.
\item The second network was trained on the classes \texttt{Model 1..Model 20}.
\item The second network was additionally re-trained to predict the class \texttt{Model $N$} in a continuous manner from a pre-trained model that was derived in Step 3.
\end{enumerate} 

All networks were based on the model with the cosine scheduler, the custom augmentations and   consistency regularization. The results of such model were presented in Figure \ref{myModel_experiment}.

Single-class training on \texttt{Model 21} was selected for the first experiment. The resuts of the evaluation performance on class \texttt{Model 21} was extracted and, ultimately, the performance was compared between all four experiments. The combined results for the single class are presented in Figure  \ref{myModel_continuous_experiment_1}.
\FloatBarrier

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./AP50_continual_21.jpg}
	\end{center}
	\caption{The \texttt{AP50} results for class \texttt{Model 21} evaluated in three different setups.}
	\begin{center}
		\label{myModel_continuous_experiment_1}
	\end{center}
\end{figure}


On the other hand, the total \texttt{AP50} results for the entire dataset (Step 1) and the total \texttt{AP50} results for classes \texttt{Model 1..Model 21} using continual learning (Step 4) are compared in Figure \ref{myModel_continuous_experiment_0}.

\todo{check this, average of 1-21 should be used to compare} 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./AP50_continual_21_allClasses.jpg}
	\end{center}
	\caption{a)The total \texttt{AP50} value evaluated on the classes \texttt{Model 1..Model 21} using continual learning b) The total \texttt{AP50} results for the classes \texttt{Model 1..Model 30} on the model that was trained from scratch.}
	\begin{center}
	\label{myModel_continuous_experiment_0}
	\end{center}
\end{figure}
\FloatBarrier


The model trained purely on the \texttt{Model 21} (Figure \ref{myModel_continuous_experiment_1} (a))  rises just as rapidly as it declines. This is contrary to the model trained on the entire dataset (Figure \ref{myModel_continuous_experiment_1} (b)), which takes longer to train but also grants better performance on the given class. Meanwhile, the model trained in a continual manner (Figure \ref{myModel_continuous_experiment_1} (c)) learns the new class almost just as fast as the model trained purely on the \texttt{Model 21}. However, after a while it stagnates and barely reaches 36\%. Similar pattern repeats for other classes of objects when Steps 2 and 4 are repeated, as can be found in Figure \ref{myModel_continuous_experiment_2}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./continualAP_average.jpg}
	\end{center}
	\caption{a) The average \texttt{AP50} value for continual learning on classes \texttt{Model 21..Model 25} given the model trained on classes \texttt{Model 1..Model 20}  b) The average \texttt{AP50} value for classes \texttt{Model 21..Model 25} extracted from the regular model trained on classes \texttt{Model 1..Model 30} c)  The \texttt{AP50} value for each of the classes \texttt{Model 21..Model 25} when trained only individually. The values are then averaged out for all 5 classes.}
	\begin{center}	\label{myModel_continuous_experiment_2}
	\end{center}
\end{figure}
\FloatBarrier

  
Here,  the Steps 2 and 4 were repeated for the classes \texttt{Model 21..Model 25}. Consequently, the average of these results for each class were combined to form the plot in Figure \ref{myModel_continuous_experiment_2} (a). For  comparison, the results of the training on the entire dataset were extracted and the plot in  \ref{myModel_continuous_experiment_2} was generated, which includes the average results between the same classes \texttt{Model 21..Model 25}. Finally, Figure \ref{myModel_continuous_experiment_2} (c) illustrates the \texttt{AP50} value for the average of each of the classes \texttt{Model 21..Model 25} when trained individually. 

As it can be concluded, training the model with more classes improves the quality of the detection from 63.97 \% to 69.66 \%. Additionally, it can be noted that the model trains significantly better when trained from the scratch \texttt{AP50} = 69.66\%, compared to training continuously (\texttt{AP50} = 43.87\%). However, it takes considerably less time to train the model continuously, where the model reaches 90 \% of the maximum \texttt{AP50} value already by 6000 iterations.

\todo{verify when the 1-30 plot is updated} 
Although the method seems to fail in learning new classes efficiently, it has proven to hold well against the catastrophic forgetting problem mentioned in Section \ref{cont_learning}, as introduced in Figure \ref{myModel_continuous_experiment_0}. 

\FloatBarrier
\clearpage
\subsection{Deployment results}
\FloatBarrier
In order to showcase the performance, a simple web app was developed. The app was hosted on a local server and it leverages Flask API to connect the detector app to the user interface. The app utilizes the model, which was presented in Figure \ref{myModel_withOrigSched} due to its relatively good performance and the fastest training speed. The prototype of the UI is presented in Figure \ref{demo}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./demo.png}
	\end{center}
	\caption{A screenshot of the simple web app. The image on the left-hand side is an uploaded target image, and the image on the right-hand side shows the predicted objects.}
	\begin{center}
		\label{demo}
	\end{center}
\end{figure}

The app sends \texttt{POST} requests to upload an image to localhost via Flask API. The uploaded image is fed to the model and returned to the user interface with the predictions (if any) after running the inference on the CPU node. The complete app is built in Python 3.9, while the model additionally utilizes packages and frameworks such as Pytorch 1.10.1, CUDA 11.3 and Detectron2 v0.6. The model was trained on Nvidia A100 units. The hardware and the computing resources were provided by CSC - Finnish IT Center for Science. 
\FloatBarrier

\clearpage