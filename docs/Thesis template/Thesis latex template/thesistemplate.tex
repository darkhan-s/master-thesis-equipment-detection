%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%% thesistemplate.tex version 3.20 (2018/08/31)                               %%
%% The LaTeX template file to be used with the aaltothesis.sty (version 3.20) %%
%% style file.                                                                %%
%% This package requires pdfx.sty v. 1.5.84 (2017/05/18) or newer.            %%
%%                                                                            %%
%% This is licensed under the terms of the MIT license below.                 %%
%%                                                                            %%
%% Written by Luis R.J. Costa.                                                %%
%% Currently developed at the Learning Services of Aalto University School of %%
%% Electrical Engineering by Luis R.J. Costa since May 2017.                  %%
%%                                                                            %%
%% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
%% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
%% Nyberg, elisabeth.nyberg@aalto.fi and Henrik Wall√©n,                       %%
%% henrik.wallen@aalto.fi.                                                    %%
%% Copyright 2017-2018 Finnish documentation in the template opinnatepohja.tex%%
%% by Perttu Puska, perttu.puska@aalto.fi, and Luis R.J. Costa.               %%
%% Copyright 2018 English template thesistemplate.tex by Luis R.J. Costa.     %%
%% Copyright 2018 Swedish template kandidatarbetsbotten.tex by Henrik Wallen. %%
%%                                                                            %%
%% Permission is hereby granted, free of charge, to any person obtaining a    %%
%% copy of this software and associated documentation files (the "Software"), %%
%% to deal in the Software without restriction, including without limitation  %%
%% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
%% and/or sell copies of the Software, and to permit persons to whom the      %%
%% Software is furnished to do so, subject to the following conditions:       %%
%% The above copyright notice and this permission notice shall be included in %%
%% all copies or substantial portions of the Software.                        %%
%% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
%% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
%% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
%% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
%% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
%% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
%% DEALINGS IN THE SOFTWARE.                                                  %%
%%                                                                            %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%%                                                                            %%
%% An example for writting your thesis using LaTeX                            %%
%% Original version and development work by Luis Costa, changes to the text   %% 
%% in the Finnish template by Perttu Puska.                                   %%
%% Support for Swedish added 15092014                                         %%
%% PDF/A-b support added on 15092017                                          %%
%% PDF/A-2 support added on 24042018                                          %%
%%                                                                            %%
%% This example consists of the files                                         %%
%%         thesistemplate.tex (version 3.20) (for text in English)            %%
%%         opinnaytepohja.tex (version 3.20) (for text in Finnish)            %%
%%         kandidatarbetsbotten.tex (version 1.00) (for text in Swedish)      %%
%%         aaltothesis.cls (versio 3.20)                                      %%
%%         kuva1.eps (graphics file)                                          %%
%%         kuva2.eps (graphics file)                                          %%
%%         kuva1.jpg (graphics file)                                          %%
%%         kuva2.jpg (graphics file)                                          %%
%%         kuva1.png (graphics file)                                          %%
%%         kuva2.png (graphics file)                                          %%
%%         kuva1.pdf (graphics file)                                          %%
%%         kuva2.pdf (graphics file)                                          %%
%%                                                                            %%
%%                                                                            %%
%% Typeset in Linux either with                                               %%
%% pdflatex: (recommended method)                                             %%
%%             $ pdflatex thesistemplate                                      %%
%%             $ pdflatex thesistemplate                                      %%
%%                                                                            %%
%%   The result is the file thesistemplate.pdf that is PDF/A compliant, if    %%
%%   you have chosen the proper \documenclass options (see comments below)    %%
%%   and your included graphics files have no problems.
%%                                                                            %%
%% Or                                                                         %%
%% latex: (this method is not recommended)                                    %%
%%             $ latex thesistemplate                                         %%
%%             $ latex thesistemplate                                         %%
%%                                                                            %%
%%   The result is the file thesistemplate.dvi, which is converted to ps      %%
%%   format as follows:                                                       %%
%%                                                                            %%
%%             $ dvips thesistemplate -o                                      %%
%%                                                                            %%
%%   and then to pdf as follows:                                              %%
%%                                                                            %%
%%             $ ps2pdf thesistemplate.ps                                     %%
%%                                                                            %%
%%   This pdf file is not PDF/A compliant. You must must make it so using,    %%
%%   e.g., Acrobat Pro or PDF-XChange.                                        %%
%%                                                                            %%
%%                                                                            %%
%% Explanatory comments in this example begin with the characters %%, and     %%
%% changes that the user can make with the character %                        %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% WHAT is PDF/A
%%
%% PDF/A is the ISO-standardized version of the pdf. The standard's goal is to
%% ensure that he file is reproducable even after a long time. PDF/A differs
%% from pdf in that it allows only those pdf features that support long-term
%% archiving of a file. For example, PDF/A requires that all used fonts are
%% embedded in the file, whereas a normal pdf can contain only a link to the
%% fonts in the system of the reader of the file. PDF/A also requires, among
%% other things, data on colour definition and the encryption used.
%% Currently three PDF/A standards exist:
%% PDF/A-1: based on PDF 1.4, standard ISO19005-1, published in 2005.
%%          Includes all the requirements essential for long-term archiving.
%% PDF/A-2: based on PDF 1.7, standard ISO19005-2, published in 2011.
%%          In addition to the above, it supports embedding of OpenType fonts,
%%          transparency in the colour definition and digital signatures.
%% PDF/A-3: based on PDF 1.7, standard ISO19005-3, published in 2012.
%%          Differs from the above only in that it allows embedding of files in
%%          any format (e.g., xml, csv, cad, spreadsheet or wordprocessing
%%          formats) into the pdf file.
%% PDF/A-1 files are not necessarily PDF/A-2 -compatible and PDF/A-2 are not
%% necessarily PDF/A-1 -compatible.
%% All of the above PDF/A standards have two levels:
%% b: (basic) requires that the visual appearance of the document is reliably
%%    reproduceable.
%% a (accessible) in addition to the b-level requirements, specifies how
%%   accessible the pdf file is to assistive software, say, for the physically
%%   impaired.
%% For more details on PDF/A, see, e.g., https://en.wikipedia.org/wiki/PDF/A
%%
%%
%% WHICH PDF/A standard should my thesis conform to?
%%
%% Primarily to the PDF/A-1b standard. All the figures and graphs typically
%% use in thesis work do not require transparency features, a basic '2-D'
%% visualisation suffices. The font to be used are specified in this template
%% and they should not be changed. However, if you have figures where
%% transparency characteristics matter, use the PDF/A-2b standard. Do not use
%% the PDF/A-3b standard for your thesis.
%%
%%
%% WHAT graphics format can I use to produce my PDF/A compliant file?
%%
%% When using pdflatex to compile your work, use jpg, png or pdf files. You may
%% have PDF/A compliance problems with figures in pdf format. Do not use PDF/A
%% compliant graphics files.
%% If you decide to use latex to compile your work, the only acceptable file
%% format for your figure is eps. DO NOT use the ps format for your figures.

%% USE one of these:
%% * the first when using pdflatex, which directly typesets your document in the
%%   chosen pdf/a format and you want to publish your thesis online,

%% * the second when you want to print your thesis to bind it, or
%% * the third when producing a ps file and a pdf/a from it.
%%
\documentclass[english, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

%% Use the following options in the \documentclass macro above:
%% your school: arts, biz, chem, elec, eng, sci
%% the character encoding scheme used by your editor: utf8, latin1
%% thesis language: english, finnish, swedish
%% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
%%                    (with pdflatex, a normal pdf containing metadata is
%%                     produced without the a-*b option)
%% typeset in symmetric layout and blue hypertext for online publication: online
%%            (no option is the default, resulting in a wide margin on the
%%             binding side of the page and black hypertext)
%% two-sided printing: twoside (default is one-sided printing)
%%

%% Use one of these if you write in Finnish (see the Finnish template
%% opinnaytepohja.tex)
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, dvips, online]{aaltothesis}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{notoccite}
\usepackage{placeins}
\usepackage{subfig}


% Select what to do with todonotes: 
% \usepackage[disable]{todonotes} % notes not showed
\usepackage[draft]{todonotes}   % notes showed

%% Math fonts, symbols, and formatting; these are usually needed
\usepackage{amsfonts,amssymb,amsbsy,amsmath}

%% Change the school field to specify your school if the automatically set name
%% is wrong
% \university{aalto-yliopisto}
% \school{S√§hk√∂tekniikan korkeakoulu}

%% Edit to conform to your degree programme
%%
\degreeprogram{Automation and Electrical Engineering}
%%

%% Your major
%%
\major{Control, Robotics and Autonomous Systems}
%%

%% Major subject code
%%
\code{ELEC3025}
%%
 
%% Choose one of the three below
%%
%\univdegree{BSc}
\univdegree{MSc}
%\univdegree{Lic}
%%

%% Your name (self explanatory...)
%%
\thesisauthor{Saidnassimov Darkhan}
%%

%% Your thesis title comes here and possibly again together with the Finnish or
%% Swedish abstract. Do not hyphenate the title, and avoid writing too long a
%% title. Should LaTeX typeset a long title unsatisfactorily, you mght have to
%% force a linebreak using the \\ control characters.
%% In this case...
%% Remember, the title should not be hyphenated!
%% A possible "and" in the title should not be the last word in the line, it
%% begins the next line.
%% Specify the title again without the linebreak characters in the optional
%% argument in box brackets. This is done because the title is part of the 
%% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
%%
\thesistitle{Equipment identification through image recognition}
%\thesistitle[Title of the thesis]{Title of\\ the thesis}
%%

%%
\place{Espoo}
%%

%% The date for the bachelor's thesis is the day it is presented
%%
\date{29.7.2022}
%%

%% Thesis supervisor
%% Note the "\" character in the title after the period and before the space
%% and the following character string.
%% This is because the period is not the end of a sentence after which a
%% slightly longer space follows, but what is desired is a regular interword
%% space.
%%
\supervisor{Prof.\ Alexander Ilin}
%%

%% Advisor(s)---two at the most---of the thesis. Check with your supervisor how
%% many official advisors you can have.
%%
\advisor{Dr. Christian Binder}
%\advisor{MSc Sarah Scientist}
%%

%% Aaltologo: syntax:
%% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
%% The logo language is set to be the same as the thesis language.
%%
\uselogo{aaltoRed}{''}
%%

%% The English abstract:
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%% Thesis keywords:
%% Note! The keywords are separated using the \spc macro
%%
\keywords{Computer vision\spc object detection\spc transfer learning\spc domain adaptation\spc continual learning}
%%

%% The abstract text. This text is included in the metadata of the pdf file as
%% well as the abstract page.
%%
\thesisabstract{
TODO
}

%% Copyright text. Copyright of a work is with the creator/author of the work
%% regardless of whether the copyright mark is explicitly in the work or not.
%% You may, if you wish, publish your work under a Creative Commons license (see
%% creaticecommons.org), in which case the license text must be visible in the
%% work. Write here the copyright text you want. It is written into the metadata
%% of the pdf file as well.
%% Syntax:
%% \copyrigthtext{metadata text}{text visible on the page}
%% 
%% In the macro below, the text written in the metadata must have a \noexpand
%% macro before the \copyright special character, and macros (\copyright and
%% \year here) must be separated by the \ character (space chacter) from the
%% text that follows. The macros in the argument of the \copyrighttext macro
%% automatically insert the year and the author's name. (Note! \ThesisAuthor is
%% an internal macro of the aaltothesis.cls class file).
%% Of course, the same text could have simply been written as
%% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
%% {Copyright \copyright{} 2018 Eddie Engineer}
%%
\copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
{Copyright \copyright{} \number\year{} \ThesisAuthor}

%% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
%% metadata to be written into the pdf file) by setting the writexmpdata switch
%% to 'false'. This allows you to write the metadata in the correct format
%% directly into the file thesistemplate.xmpdata.
%\setboolean{writexmpdatafile}{false}

%% All that is printed on paper starts here
%%



\begin{document}

%% Create the coverpage
%%
\makecoverpage

%% Typeset the copyright text.
%% If you wish, you may leave out the copyright text from the human-readable
%% page of the pdf file. This may seem like a attractive idea for the printed
%% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
%% on the page. However, the recommendation is to print this copyright text.
%%
\makecopyrightpage

%% Note that when writting your thesis in English, place the English abstract
%% first followed by the possible Finnish or Swedish abstract.

%% Abstract text
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%%
%%\begin{abstractpage}[english]
%%  Your abstract in English. Keep the abstract short. The abstract explains your
%%  research topic, the methods you have used, and the results you obtained.  
  
%%  The abstract text of this thesis is written on the readable abstract page as
%%  well as into the pdf file's metadata via the $\backslash$thesisabstract macro
%%  (see above). Write here the text that goes onto the readable abstract page.
%%  You can have special characters, linebreaks, and paragraphs here. Otherwise,
%%  this abstract text must be identical to the metadata abstract text.
  
%%  If your abstract does not contain special characters and it does not require
%%  paragraphs, you may take advantage of the abstracttext macro (see the comment
%%  below).
%%\end{abstractpage}

%% The text in the \thesisabstract macro is stored in the macro \abstractext, so
%% you can use the text metadata abstract directly as follows:
%%
\begin{abstractpage}[english]
\todo{fix this} 
Object detection is a 
\end{abstractpage}


%% Preface
%%
%% This section is optional. Remove it if you do not want a preface.
\mysection{Preface}
%\mysection{Esipuhe}
I would like to thank Professor Alexander Ilin at Aalto University for his excellent guidance. Additionally, I would like to thank Dr. Christian Binder for offering the opportunity at Metso Outotec and providing full support throughout the process. I would also like to thank my colleagues that motivated me endlessly during my internship. Finally, I would like to thank the CSC Finnish IT center for the computing resources that made the research possible. \\

\vspace{5cm}
Otaniemi, 29.7.2022

\vspace{5mm}
{\hfill Saidnassimov, D.\hspace{1cm}}

%% Force a new page after the preface
%%
\newpage


%% Table of contents. 
%%
\thesistableofcontents

\cleardoublepage
\listoffigures
\cleardoublepage

\listoftables
\cleardoublepage

%% Symbols and abbreviations
\mysection{Symbols and abbreviations}

\subsection*{Symbols}

\begin{tabular}{ll}
$\lambda $		& Regularization parameter\\
$\mathcal{L}$	& Loss function  \\
$\mathcal{W}$	& Weights matrix  \\
$\mathcal{D}$	& Domain  \\
$\mathcal{D_S}$	& Source domain dataset \\
$\mathcal{D_T}$	& Target domain dataset  \\
$\mathcal{X}$	& Feature space  \\
$\mathcal{X_S}$	& Feature space of the source domain \\
$\mathcal{X_T}$	& Feature space of the target domain \\
$\mathcal{F}$	& Feature vector\\
$\mathcal{Y}$	& Label space  \\
$\mathcal{Y_S}$	& Label space of the source domain \\
$\mathcal{Y_T}$	& Label space of the target domain \\
$P(\mathcal{X})$	& Marginal probability distribution of $\mathcal{X}$  \\
$P(\mathcal{X}, \mathcal{Y})$	& Joint distribution   \\
$P(\mathcal{Y} \mid \mathcal{X})$	& Conditional distribution  \\
$\mathcal{N}$	& Number of samples\\
$\mathcal{D}_{\mathcal{S}}=\left\{\mathcal{X}_{\mathcal{S}}^{i}, \mathcal{Y}_{\mathcal{S}}^{i}\right\}_{i=1}^{\mathcal{N}_{\mathcal{S}}}$	& Labeled source domain  \\

$\mathcal{D}_{\mathcal{T}}=\left\{\mathcal{X}_{\mathcal{T}}^{j}\right\}_{j=1}^{\mathcal{N}_{\mathcal{T}}}$		& Unlabeled target domain  \\

$\mathcal{T}$	& Task  \\
$\mathcal{T}_{n}$	& Task of iteration $n$  \\
\end{tabular}

\subsection*{Operators}
\begin{tabular}{ll}
$\displaystyle\frac{\mbox{d}}{\mbox{d} t}$ & derivative with respect to 
variable $t$\\[3mm]
$\displaystyle\frac{\partial}{\partial t}$  & partial derivative with respect 
to variable $t$ \\[3mm]
$\sum_{i=1}^{n}$                      & sum over index $i$ until $n$\\
$A \cap B$ & the intersection of two sets or areas\\
$A \cup B$ & the union of two sets or areas\\
\end{tabular}

\subsection*{List of Abbreviations}


\begin{tabular}{ll}
AI         & Artificial intelligence\\
ML         & Machine Learning\\
DL         & Deep Learning\\
GPU        & Graphical Processing Unit\\
CPU        & Central Processing Unit\\
ANN        & Artificial Neural Network\\
DNN        & Deep Neural Network\\
FC         & Fully-Connected (layer)\\
CNN        & Convolutional Neural Network\\
RCNN       & Region-based Convolutional Neural Network\\
ReLU       & Rectified Linear Unit\\
MSE        & Mean-Squared Error\\
LR 			&Learning Rate\\
SGD        & Stochastic Gradient Descent\\
PASCAL		& Pattern Analysis, Statistical Modelling and Computational Learning\\
VOC	 		& Visual Object Classes\\
COCO		& Common Objects in Context\\
ResNet		&Residual Neural Network\\
RPN			&Region Proposal Network\\
RCNN		&Regions with CNN features\\
ROI			&Region of Interest\\
FPS			&Frames Per Second\\
FPN			&Feature Pyramid Networks\\
YOLO		&You Only Look Once \\
SSD			&Single-shot MultiBox detector\\
IoU 		&Intersection over Union\\
mAP			&Mean Average Precision\\
NMS			&Non-Maxumum Suppression\\
TL			&Transfer Learning\\
DA			&Domain Adaptation\\
UDA			&Unsupervised Domain Adaptation\\
DANN		&Domain Adversarial Neural Network  \\
GAN			&Generative Adversarial Network\\
T-LESS		&Texture-LESS\\
CAD 		&Computer-Aided-Design\\
API			&Application Programming Interface\\
mAP			&mean Average Precision\\
TP			&True Positive\\
TN			&True Negative\\
FP			&False Positive\\
FN			&False Negative\\
\end{tabular}

\todo{dont forget to disable todos} 
\listoftodos

%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
%%
\cleardoublepage

%% Text body begins. Note that since the text body is mostly in Finnish the
%% majority of comments are also in Finnish after this point. There is no point
%% in explaining Finnish-language specific thesis conventions in English.
%% This text will be translated to English soon.
%%
\section{Introduction}

%% Leave page number of the first page empty
%% 
\thispagestyle{empty}
\subsection{Problem statement}
In recent years, computer vision algorithms have received much attention due to their potential applications in a vast variety of fields, including security monitoring \cite{Awalgaonkar2020}, medicine \cite{9689485}, and self-driving vehicles \cite{Janai2017, Shan2018}. However, although computer vision has been integrated into industrial applications (e.g., safety and process monitoring)\cite{Awalgaonkar2020, Banf2022}, less research has addressed the issue of industrial equipment detection \cite{Wu2022, MALBURG2021581, Kim2020}. 

As industrial plants are typically hundreds of meters long, it often becomes frustrating to identify equipment parts for maintenance or replacement. Ore processing plants treat several hundred tons of ore per hour, and the production capacity is constant. Therefore, it is often difficult to properly identify the equipment within a list of thousands of parts in a medium- to large-scale plant.

This work has been commissioned by Metso Outotec Oyj.  Metso Outotec offers digital solutions that enable customers to automate their processes in the mining, aggregates and metals industries. In order to ensure that these processes operate as smoothly as possible, it is important to optimize them at all stages of production. Recently, Metso Outotec has successfully applied computer vision in applications for identifying foreign objects in crushing processes  \cite{metso_outotec_2022}, for detecting defects in copper  molds\cite{metso_outotec_2022_2}, as well as for recognizing froth in flotation cells \cite{metso_outotec_2022_1}, to name a few. However, the company has not yet attempted to apply computer vision for facilitating maintenance. For these reasons, Metso Outotec has requested an application that would further optimize the processes by leveraging state-of-the-art computer vision algorithms in equipment recognition. 

\todo{Add missing citations}
Even though various methods have been implemented for detection of objects in a countless number of fields \cite{ima, Liu2015, He2017, Redmon2015a, Zhang2021b, Tian2019}, these methods heavily rely on extensive data collection and training of models in order to accurately identify objects. Moreover, complications arise, as it is often not possible to collect huge amounts of training images from industrial environments due to privacy and confidentiality issues. Luckily, for this project, the images can rather easily be collected from a 3D simulator model of a gold refining plant. However, using the rendered images from a 3D simulator limits the accuracy of the models, as such models do not perform as well on real images due to the domain shift phenomenon \cite{Ganin2015}, which occurs when the environmental conditions change at the time of capturing training and test images. 

Hence, this thesis proposes a cross-domain object detection approach as a solution to automatically localize and identify the equipment in a large industrial environment in order to minimize the delay in production arising as a result of manual identification. 

%https://www.mogroup.com/corporate/
%https://www.mogroup.com/corporate/about-us/

\clearpage

\subsection{Thesis objective}
\label{objective} 
The main goal of this thesis is to identify a suitable state-of-the-art object detection technique and to enhance its performance on the custom equipment dataset. The proposed method should be able to identify an object in a real image given a labeled dataset of rendered images from a 3D equipment model and a smaller unlabeled dataset of real images. Additionally, the developed method should provide a solution for optimizing the laborious process of data collection and labeling. Furthermore, the produced model should address the cases when new objects are incrementally added to the dataset.  Such optimization is important not only because training the model from the scratch is a time-demanding process, but also because large plants contain thousands of objects, thus making scalability a critical requirement. Finally, a minimal proof-of-concept application should be prepared to demonstrate the performance of the proposed detection technique.  

\subsection{Methodology}
\todo{Refine after all done}
In order to accomplish these objectives, the thesis will first explore state-of-the-art object detection frameworks, libraries and algorithms. Similarly, domain adaptation algorithms will be analyzed in an object detection setup. The most suitable methodologies will then be used to train a cross-domain object detection model.  

In order to circumvent regulations regarding accessibility and confidentiality, the dataset utilized for training the model in the experimental scenario will be based on the T-LESS open-source dataset \cite{hodan2017tless}. Since the dataset was originally intended for pose estimation in 3D models, it will be converted into formats appropriate for the proposed object detection algorithms.

To achieve higher performance in object detection, the domain shift phenomenon will be addressed using the Adaptive Teacher \cite{Li2021} algorithm for cross-domain object detection, which in turn uses the Faster-RCNN \cite{ima}  implementation in the detectron2 \cite{wu2019detectron2}  framework as a detector base.  The thesis will contribute to current knowledge by introducing an instance-level domain classifier appended to the base network of the Adaptive Teacher algorithm. Additionally, the study will evaluate the feasibility of other strategies, such as continual learning \cite{Parisi2018}, to further enhance scalability of the model. The model will then be integrated into a prototype web application for demonstration purposes. The produced model will be trained on rendered data from 3D models and evaluated on real images using mean average precision metrics. Finally, the proposed method will be evaluated using one equipment item from a real plant operated by a Metso Outotec client.
\todo{Verify if obtaining real equipment data is feasible}

\clearpage

\subsection{Scope}

The thesis will be limited to proposing a minimal proof-of-concept solution based on analyzing and combining different components of existing state-of-the-art models. In addition, this solution will be wrapped in a prototype web application. However, preparing an actual real-life dataset and implementing the solution for a real plant remains outside the scope of this study due to time constraints. Although the proposed method attempts to optimize the data collection and labeling process, this will in practice require many months before the dataset and the model based on real data would be ready for training. 

For the user interface, a prototype will be provided in order to showcase the performance of the model. However, the thesis will primarily focus on deep learning algorithms rather than methods to deploy a model in production. For this reason, the prototype will only offer basic functionality. Finally, due to time constraints, a video-compatible model will remain outside the scope of this work. 

 

\subsection{Structure of the thesis}
\todo{Check when finilizing} 
The rest of this thesis is divided into four chapters. Chapter 2 reviews the literature on object detection models, domain adaptation, the latest cross-domain object detection techniques, and class incremental learning implementations. Chapter 3 defines the dataset used and outlines the proposed architecture of the model. Chapter 4 evaluates the solution and compares the results to those of other methods using average precision metrics. Chapter 5 summarizes this work by discussing the proposed architecture and suggesting directions for future work.

%% In a thesis, every section starts a new page, hence \clearpage
\clearpage
\section{Background}
This section of the thesis introduces the key concepts related to the field of study. The section mainly discusses neural networks, object detection, trasnfer learning and domain adaptation. Additionally, the section will familiarize the reader with the relevant terminology and notations used. The section will provide an extensive overview of the latest domain-adaptive object detection methods. Finally, the topic of  continual learning will be introduced. 
\subsection{Deep learning and neural networks}
Historically, machine learning(ML), a subfield of Artificial Intelligence(AI), has been a highly computational task. The primary cause of it was linked to low hardware performance. According to the Moore's law \cite{Etiemble2022}, in a given number of months, the amount of transistors doubles in a circuit.  As the computational power of the computers grew proportianally to the number of transistors, the results have been steadily improving. The improvement was further facilitated with the discovery of the Graphical Processing Unit(GPU) applicability in ML tasks \cite{Hwang2018}. Few additional critical bottlenecks in ML are caused by suboptimal algorithms and data availability limitations. As the availability of data improved, new fields of applications arose. These and many other advancements made it possible to accelerate the training speed of deep neural networks(DNN).  

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=0.8\textwidth,height=50mm]{./ml1.png}
	\end{center}
	\caption{Machine learning concepts \cite{Janiesch2021}.}
	\begin{center}
		\label{MLConcepts}
	\end{center}
\end{figure}
\FloatBarrier

The concept of neural networks originates from biology, where a network of neurons is fundamental to the functionality of a brain. In overly simplified terms, such network consists of interconnected neurons that capture an external signal and produce a certain reaction within the brain as a response. Figure \ref{NeuronSchem}(a) illustrates a typical neuron, where the signal flows from dendrites through the cellbody of the neuron. If the signal is strong enough, the neuron activated and passes the signal further to other neurons through the connections called "synapses", as shown in Figure \ref{NeuronSchem}(c). Identical process takes place in remaining neurons, which ultimately forms a neural network \cite{Mehlig_2021}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=8cm]{./BioNeuronVsArtificial.png}
	\end{center}
	\caption{A biological(a) neuron against artificial(b) and biological synapse(c) against artificial(d)\cite{article1}.}
	\begin{center}
		\label{NeuronSchem}
	\end{center}
\end{figure}
\FloatBarrier

In deep learning(DL), a subfield of ML, this architecture has been borrowed to implement an artificial neural network(ANN), where a neuron is simply a unit that processes an input signal. Figure \ref{NeuronSchem}(b) demonstrates a simplified structure of an artificial neuron. Here, $x_1, x_2..x_n$ represent input signals, while $w_1, w_2, ... w_n$ are the weights of the signal. The higher the weights of the input are, the stronger the influence of the neuron on the output. The weighted sum of the inputs is then passed to the activation function, which essentially determines the output of the node and allows to learn complex patterns in data \cite{Mehlig_2021}. 

A few of the most popular non-linear activation functions include a logistic sigmoid, tanh function, softmax and a rectified linear unit(ReLU). Among the four, ReLU has been considered state-of-the-art in the field of deep learning due to the performance in convolutional neural networks(CNN) \cite{Dubey2021} and the simplicity. The logic of ReLU can be represented as follows: 
  
\begin{equation}
	\operatorname{ReLU}(x)=\max (0, x)= \begin{cases}x, & \text { if } x \geq 0 \\ 0, & \text { otherwise }\end{cases}
\end{equation}

Consequently, the output of the activation function is passed to a hidden layer of neurons, as illustrated in Figure \ref{NeuronSchem}(d). The layers in the middle are called hidden due to the fact that both outputs and inputs are masked by the activation function.  The hidden layers will calculate the weighted output of the previous layers until the signal eventually reaches the final output layer of the network. Hidden layers that stack up together to form a classical deep learning architecture \cite{OShea2015}. Such architecture allows to process data in a non-linear pattern. In the original ANN all the layers are fully-connected(FC), meaning that each node of the input vector affects each node of the output vector, as shown in Figure \ref{NeuronSchem}(d).


Due to the biological nature, neural networks adapt over time by creating new connections between neurons. The neurons in ANN adopted such behaviour by utilizing a backpropogation algorithm \cite{Rumelhart:1986we}. A naive backpropogation approach is illustrated in Figure \ref{fig:backprop}. The algorithm consists of two parts - feed-forward and backward loops. Generally, the main objective of an ANN is to choose such weights that the network produces desired target outputs. The forward pass propagates along the nodes in each layer of the neural network and returns a predicted output. In order to evaluate the quality of the predicted output, it is compared to the target output by using a cost(loss) function. The classic example of a cost function is a mean-squared error (MSE), which is commonly used in regression based problems. The equation to MSE is shown in Equation \ref{MSE_eq}.

\begin{equation}
\underset{\theta}{\operatorname{argmin}} \frac{1}{m} \sum_{i=1}^{m}\left(f_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}
\label{MSE_eq} 
\end{equation}

The MSE cost function attempts to minimize the distance between the predicted output and the target output, while giving more weight to larger distances due to the squared output \cite{Albarghouthi2021}.


\begin{figure}[htb]
    \centering
    \subfloat{{\includegraphics[width=10.5cm]{./Backprop1.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=1.5cm]{./Backprop2.png} }}%
    \caption{Backpropogation algorithm, adapted from \cite{Alber2018}.}
    \label{fig:backprop}%
\end{figure}
\FloatBarrier

In order to minimize the loss function, the algorithm calculates the partial derivative of the loss term $L$ with respect to the weights: $\frac{\partial L(f(x), y)}{\partial W_{i}}$. An algorithm of the gradient descent is commonly used in optimizing such functions and its logic can be generalized as follows: 

\todo{Make sure this formula makes sense with the rest of the text}
\begin{enumerate}
  \item Start with $j=0$ and a random value of $\theta$, called $\theta^{0}$.
  \item Set $\theta^{j+1}$ to $\theta^{j}-\eta\left((\nabla g)\left(\theta^{i}\right)\right)$.
  \item $j$ to $j+1$ and repeat \cite{Albarghouthi2021}.
\end{enumerate} 

\todo{Todo: write about learning rate, overshooting, local/global minima}
Generally speaking, there are three gradient descent algorithms: batch gradient descent, stochastic gradient descent(SGD) and mini-batch gradient descent. While the batch gradient descent updates the model only after all the samples have been evaluated, the SGD calculates the error for one sample in the dataset and updates the parameters one at the time. On the other hand, the mini-batch gradient descent algorithm splits the data into smaller batches and calculates updates on each of the data subsets. 

Finally, by applying the chain rule to the derivatives in a backwards direction, the updates $\Delta W$ are calculated for each of weights the in the layers \cite{Alber2018}. The algorithm is then repeated until convergence. The process of determining the weight values to utilize in each subsequent layer in the neural network by means of backpropogation algorithm is called "training the model".  

\subsection{Neural networks in computer vision}
\label{neural_nets} 
With the discovery of DNN, many of the popular computer vision techniques became obsolete. Specifically, the introduction of CNNs was an important milestone in boosting machine perception performance \cite{Mahony2019}. Nowadays, CNNs are typically used to tackle various pattern recognition and computer vision tasks. Some of the tasks  include:

\begin{itemize}
	\item Image Classification
	\item Object Detection
	\item Segmentation
	\item Facial Recognition and Modelling
	\item Domain Adaptation
	\item Image Reconstruction
	\item and with many others \cite{paperswithcode:2022}.
\end{itemize}

For addressing the objectives of this work, the thesis will extensively cover image classification, object detection and domain adaptation tasks.

Figure \ref{CNN} illustrates a simplistic CNN architecture approach to the MNIST  \cite{lecun-mnisthandwrittendigit-2010} classification problem. A CNN is essentially a neural network that leverages convolutional layers to produce predictions. Unlike the traditional computer vision methods, CNNs do not need to extract features of the image beforehand due to the logic behind convolution. In classical ML the features are extracted separately, followed by the appropriate algorithms for learning. On the countrary, DL algorithms, such as CNN, learn the features automatically \cite{alom01}.


\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=5cm]{./CNN.png}
	\end{center}
	\caption{A simple CNN network with 5 layers for image classification task \cite{Mahony2019}.}
	\begin{center}
		\label{CNN}
	\end{center}
\end{figure}
\FloatBarrier

To understand the logic of convolutional layers, it is important to discuss the operation of convolution. Convolution  is a mathematical operation of two functions that indicates how the shape of one affected by another. In terms of image processing, convolution is a process, where the kernel moves along the input matrix dimensions. Each output pixel can then be calculated as the dot product of the cropped input and the kernel \cite{Liu2016}. Figure \ref{CNN_1}(a) illustrates the process of convolution in image processing.  
\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=5cm]{./CNN_1.png}
	\end{center}
	\caption{The process of convolution \cite{Liu2016}.}
	\begin{center}
		\label{CNN_1}
	\end{center}
\end{figure}
\FloatBarrier

As a result, the elements of the network  are not densely connected, which allows better generalization and flexibility. This operation in practice allows to extract the important features of the image, such as edges, corners, shapes and many others.  However, unlike in the FC layers, the number of weights is much smaller, which is essential when it comes to high-dimensional images. Following the architecture in Figure \ref{CNN}, the outputs of the convolutional layers are activated with ReLU in a similar manner as in a classical ANN structure. The outputs are then pooled in order to downsample the image and hence reduce the computational costs \cite{Liu2016}. The commonly used max-pooling operation is shown in Figure \ref{CNN_1}(b). Finally, fully-connected layers complete the structure of the basic CNN. FC layers are used to flatten the ouputs from the convolutional layers. This in turn allows, in case of the input in Figure \ref{CNN}, to compute the probability of the input image to represent a number \cite{Mahony2019}.

CNNs boosted the performance in computer vision tasks. However, deep learning methods still required extensive training data. Luckily, multiple algorithms emerged as large datasets became available. The datasets were made public as different image classification challenges appeared. The  datasets commonly used in benchmarking are ImageNet \cite{Russakovsky2014}, PASCAL Visual Object Classes(VOC)\cite{Everingham10} and Common Objects in Context(COCO) \cite{Lin2014}.

\subsection{Image classification}
It is important to understand the concepts of image classification before moving on to object detection principles. Among the three mentioned earlier, ImageNet was chosen to be a de-facto dataset for running benchmarks.  Different CNN-based models were proposed and some of the most popular models include LeNet \cite{lecun-gradientbased-learning-applied-1998}, AlexNet \cite{NIPS2012_c399862d}, VGGNet \cite{Simonyan2014} and Residual Neural Network(ResNet) \cite{He2015}. As it can be concluded from Figure \ref{image_net}, the classification error dropped lower than the error from the human eye with the introduction of ResNet, thus approaching the theoretical limits. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=5cm]{./image_net.png}
	\end{center}
	\caption{Evolution of image classifier models evaluated on ImageNet dataset \cite{alom01}.}
	\begin{center}
		\label{image_net}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{LeNet}
LeNet architecture (1998) \cite{lecun-gradientbased-learning-applied-1998} is considered to be a pioneer in the field. Its design inherited the classic CNN architecture, although instead it consisted of 7 layers, as presented in Figure \ref{LeNet}. However, the implementation of the paper was not possible for more than 10 years due to limitations in computing power. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=4cm]{./LeNet.png}
	\end{center}
	\caption{LeNet architecture \cite{alom01}.}
	\begin{center}
		\label{LeNet}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{AlexNet}
Consequently, AlexNet paper was introduced, which proved the effectiveness of their model, as it outperformed the state-of-the-art implementations and achieved the error rate of 15.3\% \cite{NIPS2012_c399862d}. Figure \ref{AlexNet} displays the proposed network. The architecture of AlexNet is similar to one of LeNet, though it is substantially deeper and has more than 60 million adjustable parameters. It has 5 convolutional layers of varying kernel size. The convolutional layers are followed byt ReLU activation functions and max-pooling layers. The architecture is finalized by attaching two FC layers with dropout rate of 0.5 and one softmax layer.

During dropout, there is a probability that the neuron will be excluded from computations in the subsequent layer. Utilizing such technique proved to be essential to fight overfitting in FC layers and improve generalization  \cite{JMLR:v15:srivastava14a}. 

Overall, AlexNet architecture was the first CNN to split the model into two parts and to leverage mulitple GPUs in training due to GPU memory limitations of 3GB at the time. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=4cm]{./AlexNet.png}
	\end{center}
	\caption{AlexNet architecture \cite{alom01}.}
	\begin{center}
		\label{AlexNet}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{VGG}
Another milestone was achieved with the discovery proposed in VGGNet \cite{Simonyan2014}. This work proved that the depth of the network produces a significant impact on the performance of the CNN in classification tasks \cite{alom01}. Three different versions of the model were proposed with 11, 16 and 19 layers, respectively and with the deeper model being the best in performance, but more expensive in terms of computation. Equivalently to AlexNet, the network has blocks of convolutional layers of a mixed kernel size, followed by a ReLU block and max-pooling. The network is finilized with three FC and one softmax layers. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=4cm]{./vgg.png}
	\end{center}
	\caption{VGG architecture \cite{alom01}.}
	\begin{center}
		\label{VGG}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{ResNet}
ResNet architecture \cite{He2015}, which was proposed in 2015, discovered that after certain depth, the performance of the model degrades. He et al. suggested that this happens due to the "vanishing gradient" problem. As the model gets deeper, several applications of the chain rule on during backpropogation tend to diminish either all the way to zero or becomes too large. As a result, no update is applied on the weights and hence, no training takes place. He et al. proposed to utilize a residual block, illustrated in Figure \ref{resnet}, which is used to skip some of the layers in between.  This solution essentially mitigates the problem of vanishing gradients by allowing the training loop to skip parts of the network that affect the performance negatively. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=4cm]{./resnet.png}
	\end{center}
	\caption{Residual block of ResNet\cite{He2015}.}
	\begin{center}
		\label{resnet}
	\end{center}
\end{figure}
\FloatBarrier

ResNet adopts the VGG-19 architecture, with an exception of the skip connection block added. He et al. implemented multiple versions of the model, including one that is 152 layers deep. Nonetheless, the network has less trainable parameters, which substantially improves the training speed while preserving accuracy due to the residual block. As a result, the proposed model achieved the error of 3.57\% on ImageNet dataset, which is lower than the human error \cite{He2015}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./resnetFull.png}
	\end{center}
	\caption{ResNet architecture\cite{resnet50}.}
	\begin{center}
		\label{resnetFull}
	\end{center}
\end{figure}
\FloatBarrier


%Often, image processing tasks are classified into three main learning paradigms - supervised,  %unsupervised and semi-supervised learning.


\subsection{Object detection}
\label{obj_detection_section} 
The problem of object detection is an extended version of the image classification problem. However, unlike image classification, object detection aims to recognize not only the object, but also to localize it. Prior to arrival of deep learning, object detection considered to be a difficult task. With the discovery of the CNN architecture, the traditional computer vision techniques became obsolete. Since then, multiple different detector algorithms emerged. A typical object detection network contains two important modules - the base, or backbone network, and the detector network. A base network is usually one of the pretrained VGG or ResNet models, presented in the previous chapter. A base network acts as a feature extractor, and the features are then passed to a detector. Generally speaking, deep learning based detector networks can be classified into two categories: single- and two-stage detectors \cite{Zaidi2021}. Two-stage object detectors attempt to propose regions of the image that contain an object in the first step, and then run the task of classification and localization on the given proposal region. On the other hand, single-stage detectors try to detect objects directly without running a Region Proposal Network (RPN). Figure \ref{fig:1vs2stage} illustrates the key differences in the structure of the two categories. 

\begin{figure}[htb]
    \centering
    \subfloat{\raisebox{0.13\height}{\includegraphics[width=6cm]{./1stage.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=6cm]{./2stage.png} }}%
    \caption{A simple single-stage detector(left) compared to a two-stage detector(right)\cite{app8091488}.}
    \label{fig:1vs2stage}%
\end{figure}
\FloatBarrier

Typical examples of the single-stage networks are RetinaNet \cite{Lin2017}, Single-shot detector(SSD) \cite{Liu2015} and You only look once(YOLO) \cite{Redmon2015a}. In two-stage detectors, Region-based CNN(R-CNN) \cite{Girshick2013}, Fast-RCNN \cite{Girshick2015} and Faster-RCNN \cite{ima} are considered to be the most important discoveries. Figure \ref{OD} also mentions detectors such as FCOS \cite{Tian2019}, Mask-RCNN \cite{He2017} and DETR \cite{Carion2020}. Although they show competitive performance, they are not reviewed in the thesis as they are not relevant to the method introduced in the \nameref{Methodology}.  The following subsection will introduce the reader to some of the main concepts of the selected detectors. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./OD.png}
	\end{center}
	\caption{Types of object detectors}
	\begin{center}
		\label{OD}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{R-CNN}

Figure \ref{rcnn} shows the structure of the R-CNN network \cite{Girshick2013}. R-CNN has been one of the first CNN-based models to be introduced \cite{Zaidi2021}. Girshick et al. essentially suggested to use an module that extracts the object proposals and then to pass it to a CNN. The CNN would then extract the features relevant, which would in turn allow to classify the proposed region as well as to localize it. In their original experiments, the selective search algorithm  \cite{Uijlings13} was used to produce roughly 2000 regions. AlexNet  \cite{NIPS2012_c399862d} was used as a backbone CNN to extract vectors of 4096 size dimensions. The features were then passed to binary classifers that are bound to a certain class. As multiple regions are returned, non-maxumum suppression(NMS) \cite{Hosang2017}  is used to identify the best proposal for the object. 

Unfortunately, the inference process in R-CNN took a whole 47 seconds per image \cite{Girshick2013}, which was not fast enough to be remain relevant in object detection field. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=12cm]{./rcnn.png}
	\end{center}
	\caption{R-CNN overview \cite{Girshick2013}.}
	\begin{center}
		\label{rcnn}
	\end{center}
\end{figure}


\subsubsection{Fast-RCNN}
\todo{Write more about ROI}
Unlike the classic R-CNN system, where the components such as the classifier and the regressor have to be trained separately, the same authors of the RCNN  model also introduced a Fast-RCNN model \cite{Girshick2015}, which proposed an end-to-end trainable system. Additionally, the region-of-interest(ROI) pooling layer was proposed. The architecture of the Fast-RCNN shown in Figure \ref{fast_rcnn} below. The ROI layer receives max-pooled feature maps that were generated by the backbone CNN and then for every proposed region, a fixed size feature vector is extracted from the map. This allows the rest of the network to focus purely on the features extracted for the proposed regions. Finally, two additional FC output layers are appended, where the first one classifies the features by returning one of the object classes K in range (0, K+1], thus including the background class as well. The other one returns 4 real values for each feature vector, which denote the boudning box corner coordinates excluding the background class \cite{Girshick2015}. Fast-RCNN showed a significant(x146) speed improvement as compared to the R-CNN, thus allowing it to be used in real-time applications \cite{Girshick2013}.
\todo{Write more about image pyramids}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=10cm]{./fast_rcnn.png}
	\end{center}
	\caption{Fast-RCNN overview\cite{Girshick2015}.}
	\begin{center}
		\label{fast_rcnn}
	\end{center}
\end{figure}
\FloatBarrier

\subsubsection{Faster-RCNN} 
\todo{Perhaps write in greater detail}
Ren et al. \cite{ima} discovered that even though Fast-RCNN was significantly faster than the precessor networks, this architecture still had a bottleneck remaining in its region proposal counterpart. Therefore, in the novel Faster-RCNN \cite{ima} it was suggested to replace the legacy region proposal module with a fully-convoluted architecture called region proposal network(RPN) \cite{Girshick2013}. Instead of using image pyramids to solve the problem as it was proposed by Girshick et al. \cite{Girshick2015}, Faster-RCNN utilizes anchor boxes of different aspect ratios to propose object candidates. Similarly to any other CNN, the features in Faster-RCNN are extracted from the convolutional layers based on a VGG backbone. The features maps are then sent to the RPN, which in practice is a sliding window  of $n\times n (n=3)$ dimensions. At every sliding window position, the $k$ number of object proposals are predicted. Such boxes are called "anchors" as illustrated in Figure \ref{fig:faster_rcnn}. The acquired $2k$ classification predictions whether the proposed region is an object of interest or not and $4$ regression outputs of the bounding box coordinates are then mapped back to the ROI layer and, eventually, to the FC layers that predict the proposed object's class \cite{ima}. 


\begin{figure}[htb]
    \centering
    \subfloat{{\includegraphics[width=6cm]{./faster_rcnn.png} }}%
    \qquad
    \subfloat{\raisebox{0.5\height}{\includegraphics[width=6cm]{./faster_rcnn_rpn.png} }}%
    \caption{Faster-RCNN overview and its RPN module \cite{ima}.}
    \label{fig:faster_rcnn}%
\end{figure}
\FloatBarrier

As the network returns the predictions, the loss function is calculated as in Equation \ref{faster_rcnn_loss}:  
\begin{equation}
\begin{array}{r}
L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right)=\frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{*}\right) \\
+\lambda \frac{1}{N_{\text {reg }}} \sum_{i} p_{i}^{*} L_{r e g}\left(t_{i}, t_{i}^{*}\right)
\end{array}
\label{faster_rcnn_loss} 
\cite{ima},
\end{equation}

where $p_i$ is a probability of the predicted anchor $i$ to be an object,  $p_{i}^{*}$ is a ground-truth label from 0 to 1 , $t_i$ is a vector with 4 coordinates of the bounding box and $t_{i}^{*}$ is its corresponding ground-truth vector. $\lambda$ is a trade-off between the $L_{cls}$ and $L_{reg}$, where $L_{c l s}$ is calculated as a binary log loss and $L_{reg}$ uses a smooth $L_1$ loss that was introduced in Fast-RCNN \cite{Girshick2015}.  




This implementation, unlike Fast-RCNN, allowed to return predictions nearly in real time with the breakthrough of 5 frames per second(FPS) \cite{ima}.
Although the two-stage Faster-RCNN detector is nearly 7 years old at the time of writing this thesis, it is still one of the most widely used detectors in the field, as can be noticed from Figure \ref{popularity}. In later chapters, this work will focus on the Faster-RCNN implementation. Nevertheless, it is worth mentioning single-stage competitors. Often they are slightly weaker in terms of accuracy, compared to two-stage detectors. However, these detectors offer a significant improvement in real-timeness as compared even to the fastest one of the two-stage detectors, Faster-RCNN. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./detectorsPopularity.png}
	\end{center}
	\caption{The popularity of different detectors according to  \cite{paperswithcode_1:2022}.}
	\begin{center}
		\label{popularity}
	\end{center}
\end{figure}
\FloatBarrier


\subsubsection{YOLO}
\label{yolo_section} 

Unlike the two-stage methods presented, You Only Look Once(YOLO) \cite{Redmon2015a} algorithm attempts to solve the object detection problem purely as a regression problem by predicting the bounding boxes of the objects directly without region proposals. The YOLO network instead splits the image into a grid of $S\times S$ cells, for which a $B$ number of boundning boxes and $C$ probabilities of the class and  are predicted \cite{Redmon2015a}. The overview of the detection process is shown in Figure \ref{yolo}. The principle of the YOLO grid component is broadly similar the one of R-CNN \cite{Girshick2013}, where the algorithm of selective search \cite{Uijlings13} is used to propose regions. However, instead of proposing more than 2000 regions, YOLO only returns 98 proposal boxes per image. This, along with having an optimized single-stage detection process, allowed YOLO to achieve impressive nearly real-time FPS results. Redmon et al. reported YOLO to sustain the average FPS of 45, while the most accurate version of the Faster-RCNN network had 7 FPS \cite{Redmon2015a}. However, the proposed network still has multiple limitations, which were addressed in later iterations of the paper  \cite{Redmon2016, Redmon2018a}.

The architecture consists of 24 cascaded convolutional and 2 FC layers.The convolutional layers are first pretrained on the ImageNet dataset \cite{Russakovsky2014}. The image size is reduced by applying $1\times1$ convolutional layers in between, also referred to as "reduction layers"  \cite{Redmon2015a}. The simplified architecture is presented in Figure \ref{ssd}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=12cm]{./yolo.png}
	\end{center}
	\caption{YOLO overview\cite{Redmon2015a}.}
	\begin{center}
		\label{yolo}
	\end{center}
\end{figure}
\FloatBarrier


%%\subsubsection{RetinaNet}

%%\begin{figure}[htb]
%%	\begin{center}
%%		\includegraphics[width=14cm]{./retinaNet.png}
%%	\end{center}
%%	\caption{RetinaNet overview\cite{Lin2017}}
%%	\begin{center}
%%		\label{retinanet}
%%	\end{center}
%%\end{figure}
%%\FloatBarrier


\subsubsection{SSD}
Another implementation of a single-stage network worth mentioning is a Single-Shot MultiBox detector(SSD) \cite{Liu2015}. The differences in architectures of SSD and YOLO are displayed in Figure \ref{ssd}. Liu et al. proposed a model that detects objects in real-time while preserving the accuracy. First, the image is fed into the backbone CNN, in the original experiments - VGG-16. The SSD head layers added after the backbone network are convolutional as well. Similarly to YOLO, the image is split into a grid of $n\times n$ size, where it benetits from the anchor boxes of varying aspect ratio. 


\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./ssd_yolo.png}
	\end{center}
	\caption{SSD compared to YOLO\cite{Liu2015}.}
	\begin{center}
		\label{ssd}
	\end{center}
\end{figure}
\FloatBarrier

However, as the objects might not always be within the grid boundaries, as can be noticed from Figure \ref{ssd_boxes}. Therefore, SSD paper introduced a concept of anchor boxes with an offset. The anchor boxes with offsets that have the highest overlap with the ground truth box of the object are then oassed to FC layers to predict the class and the location of the object. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=12cm]{./ssd_boxes.png}
	\end{center}
	\caption{SSD anchor boxes \cite{Liu2015}.}
	\begin{center}
		\label{ssd_boxes}
	\end{center}
\end{figure}
\FloatBarrier

The key object detection models of the last decade are presented in the Table \ref{detectorsTable} below. As it can be deducted from the table, Faster-RCNN shows the best performance on the PASCAL VOC dataset, while YOLO demonstrated the highest FPS. The survey of Zaidi et al. \cite{Zaidi2021} also discusses lightweight models. However, for the purpose of this study, the detection accuracy is the primary focus rather than speed, thus their review will be omitted. 

\begin{table}[htb]
	\captionof{table}{Overview of object detectors\cite{Zaidi2021}. }
	\begin{center}
		\includegraphics[width=16cm]{./detectorsTable.png}
	\end{center}
	\begin{center}
		\label{detectorsTable}
	\end{center}
\end{table}
\FloatBarrier

As it can be seen from the Table \ref{detectorsTable}, there are multiple models that are not covered in this section. Additionally, there are several detectors that show promising results in the domain-adaptive object detection setup, such as FCOS \cite{Tian2019} and DETR \cite{Zhang2021b}. However, as their performance is not yet extensively evaluated, they will be omitted from this thesis. 

\subsection{Object detection in industrial environments}
Although object detection is an extremely diverse field with applications varying from medicine \cite{9689485} to self-driving vehicles \cite{Janai2017, Shan2018}, the industrial object detection is mostly limited to safety and process monitoring \cite{Awalgaonkar2020, Banf2022}. Fewer research has addressed the topic of industrial object detection. In general, many of the introduced object detection methods can be applied to this problem directly as it is. Malburg et al. \cite{MALBURG2021581} has evaluated multiple different versions of object detection models such as YOLO \cite{Redmon2015a} in an industrial setup. However, often in industrial environments severe challenges such as overlapping and occlusion take place. Moreover, the object instances often are represented in multiple scales. Therefore, it is often impossible to obtain reliable results using a standard object detector architecture. Wu et al. \cite{Wu2022} has recently addressed the problem of industrial object detection using an R-CNN based approach that leverages Feature Pyramid Networks to improve detection at multiple scales and NMS with penalty factors to eliminate occlusion problem. However, the proposed method does not consider the data availability limitations and confidenciality issues that were outlined in the \nameref{objective}. On the other hand, Kim et al. \cite{Kim2020} minimized the need for labeling from a human by utilizing a deep active learning approach. While the core of this approach is a standard Faster-RCNN \cite{ima} network, Kim et al. also proposed to utilize uncertainty evaluation in order to enable the model to label images interactively over time. As the training advances, the model evaluates whether the predicted bounding boxes are reliable or not using the sum of entropy for each bounding box. In the subsequent stage the model requests the user to manually label 10\% of the data with highest uncertainty. The model is then retrained with the updated knowledge. Although this approach seems promising, it  does not consider the domain shift phenomenon that takes place in the setup defined by the thesis objectives. Later sections of the thesis will address this phenomenon. 

\todo{Write more about the active learning method if there is time} 


\subsection{Transfer learning}
\label{transferLearning} 
In this section, the multiple transfer learning(TL) techniques will be introduced. Despite the fact that the methods discussed in previous chapters have been successfully implemented in various scenarios, ML in general often struggles to apply such methods in real life. This is normally caused by insufficient training data, as it is often expensive to collect it, both in terms of time, financially, and sometimes simply not possible at all \cite{Zhuang2019}. Moreover, the requirement of having to train the models on new massive datasets often make ML solutions inefficient in practice. For this reason, TL concepts have been found advantageous as it addresses transferring knowledge learned from one task, domain or distribution to another. Here and in the subsequent sections, the notations are identical to those outlined by Pan et al. \cite{Pan2010} and are introduced as follows:  

\todo{mention later: Zhuang2019: TL does not always bring benefit}

\begin{itemize}
\item A domain $\mathcal{D}$ can be defined as a composite term, which is characterized by two elements: feature space $\mathcal{X}$ and a marginal probability distribution $P(X)$; $X=\left\{x_{1}, \ldots, x_{n}\right\} \in \mathcal{X}$. With this being said, two domains are defined as different if their feature spaces or marginal probability distributions are different. In this thesis, the domains are restricted with one source $\mathcal{D_S}$ and one target $\mathcal{D_T}$ domains \cite{Pan2010}. 
\item Similarly, a task $\mathcal{T}$ can be defined by its label space $\mathcal{Y}$ and a conditional probability of $\mathcal{Y}$ given $X$, e.g. $P(Y \mid X)$, $Y=\left\{y_{1}, \ldots, y_{n}\right\} \in \mathcal{Y}$. In practice, a conditional probability is defined as a function that can learn to predict a label $y_i$ given a sample vector $x_i$ \cite{Pan2010}. 
\end{itemize} 


 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./TL.png}
	\end{center}
	\caption{Comparison of ML to TL\cite{Pan2010}.}
	\begin{center}
		\label{TL}
	\end{center}
\end{figure}
\FloatBarrier

Sun et al. \cite{SUN201584} describes TL as a method to transfer knowledge when the feature space between the two given source and target domains is different, meaning that $\mathcal{X}_{S} \neq \mathcal{X}_{T}$. Pan et al. \cite{Pan2010} mentions that transfer learning can be also defined when the marginal probability is different, meaning that $P_{S}(X) \neq P_{T}(X)$. Similarly, transfer learning can be applied when $\mathcal{T}_{S} \neq \mathcal{T}_{T}$. An example of a simple TL problem is training a model that was trained for classifying cats, but instead is  required to learn a new task such as classifying dogs. However, in various scenarios, the task is essentially the same but the domain is similar yet different. For instance, a model that was trained to identify cats in sketches, is instead required to identify cats in real images. Solving such tasks is the main focus of domain adaptation(DA). Often the terms "domain adaptation" and "transfer learning" are used interchangeably. However, according to Wang et al. \cite{Wang2018} and Zhang et al. \cite{Zhang2021}, domain adaptation(DA) is a special case of TL. In scientific terms, the problem that DA attempts to solve can be defined when the source and target feature spaces are the same $\mathcal{X}_{S}=\mathcal{X}_{T}$, but the marginal probability distribution is not $P\left(\mathcal{X}_{S}\right) \neq P\left(\mathcal{X}_{T}\right)$\cite{SUN201584}. This is not to be confused with semi-supervised machine learning, where both labeled and unlabeled data is typically supplied from one domain \cite{SUN201584}. 

\todo{Talk about the comment here perhaps} 
%https://www.v7labs.com/blog/transfer-learning-guide


\subsubsection{Domain adaptation}
\label{DA_section} 
DA has lately been gaining popularity in both image classification and object detection tasks \cite{Zhang2021}. Typically, DA is used to predict a label given the data from a source domain and limited or no data from the target domain. Most importantly, DA addresses the domain shift problem \cite{Zhang2021}. In a DA problem, a domain shift, also known as a distributional shift or dataset bias, can be defined as a change in distribution of data between source and target domains. 


Zhang et al. \cite{Zhang2021} classify DA methods into three categories that are similar to ML types - supervised, semi-supervised, and unsupervised DA. Alternatively, Oza et al. \cite{Oza2021} classify the collected DA methods into semi-supervised, weakly-supervised and unsupervised DA. Indeed, supervised DA is not commonly used since the primary goal of DA is to reduce the domain shift when the data availability is limited. 

According to Zhang et al. \cite{Zhang2021}, different methods that attempt to solve the distribution shift problem by minimizing the distance between marginal, conditional or joint distributions. 

The visual representation of these distribution alignment types are shown in Figure \ref{distribution} and the methods in the following subsections attempt to minimize the domain shift by one way or another. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=12cm]{./distribution.png}
	\end{center}
	\caption{Distribution alignment types \cite{Zhang2021}.}
	\begin{center}
		\label{distribution}
	\end{center}
\end{figure}
\FloatBarrier 



\subsection{Domain adaptive object detection}
\label{DAobj} 
Next subsection will discuss some of the methods that were proposed at different times to solve the problem of domain shift in object detectors. Oza et al.  \cite{Oza2021} have extensively reviewed and grouped the existing approaches into following six categories:
\begin{enumerate}
	\item Adversarial feature learning
	\item Pseudo-label based self-training
	\item Image-to-image translation
	\item Domain randomization 
	\item Mean-teacher training
	\item Graph reasoning
\end{enumerate} 

Many of the methods collected by Oza et al. overlap with each other and fall into more than one group. In this thesis, a few methods will be briefly reviewed for each of the categories above. Some of the methods are listed in Figure \ref{UDA_OD}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./UDA_OD.png}
	\end{center}
	\caption{Unsupervised Domain Adaptative Object Detection}
	\begin{center}
		\label{UDA_OD}
	\end{center}
\end{figure}
\FloatBarrier

In the following methods, it will be assumed that $\mathcal{D_S}$ and $\mathcal{D_T}$ originate from similar yet different distributions. Additionally, the papers introduced in the following section mainly address the more complex, unsupervised domain adaptation(UDA) problem.

\subsubsection{Gradient reversal layer}

One of the key components in a typical domain adaptive setup for both image classification and object detection problems is a gradient reversal layer(GRL) that has been proposed by Ganin et al. \cite{Ganin2015}. The authors suggest that in order to successfully solve the domain adaptation problem, the prediction should be based on the features that cannot differentiate between the source and target domains. In other words, the network should propose features that are common for both domains. Figure \ref{DANN} illustrates the Domain-Adversarial Neural Network, or DANN \cite{Ganin2015}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./GRL.png}
	\end{center}
	\caption{Domain-adversarial neural network and GRL \cite{Ganin2015}.}
	\begin{center}
		\label{DANN}
	\end{center}
\end{figure}
\FloatBarrier
The network is essentially a simple feed-forward network with a feature extractor and a predictor that classifies the input. Two additional components are appended to the last layer of the feature extractor - a gradient reversal layer(GRL) and a domain classifier. On the forward pass of the DANN, the network attempts to predict the class and the domain labels and GRL acts as an identity function. During the backpropogation, the GRL multiplies the gradient of the domain classifier by a fixed negative weight constant $\lambda$. This enables the domain classier to maximize the domain classification loss and, therefore, "confuse" the feature extractor and force it to generate only domain invariant features \cite{Ganin2015}. 
\todo{Perhaps add more math about GRL} 


\subsubsection{Adversarial feature learning}
\label{adv_approach} 

Although DANN is an implementation of a domain adaptive image classification, GRL is a fundamental component in adversarial feature learning of object detectors and will be referenced in the subsequent sections of the thesis. 

The majority of the methods, collected by Oza et al.\cite{Oza2021}, are based on the two-stage object detectors and Faster-RCNN \cite{ima} in particular. This has been presumably facilitated by Pytorch \cite{NEURIPS2019_9015} package in Python and frameworks such as Detectron \cite{Detectron2018} and Detectron2 \cite{wu2019detectron2} that enabled researchers to extend the possibilities of Faster-RCNN and improve its scalability. With the arrival of the mentioned tools and their pre-trained models, Faster-RCNN became a good starting point to experiment with new tasks, which was done by replacing backbone networks and adding new components. 

\todo{Write more about this as it is key of your method} 
One of the first implementations of such approach in object detectors is presented in the paper by Chen et al. \cite{Chen2018}. The proposed architecture is based on the Faster-RCNN object detector. As it can be seen from Figure \ref{Faster_rcnn_DA}, this method proposed to apply adversarial learning at multiple stages of the detection by applying GRL strategy, namely in the image- and instance-level of the network. To be more precise, a GRL and a domain classifier are appended to the extracted feature map, same way as in Figure \ref{DANN} to form an image-level domain classifier. Similarly, regions predicted by the FC layers follow an equivalent procedure. Finally, the classifiers are regularized using consistency loss.   

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./faster_rcnn_DA.png}
	\end{center}
	\caption{Domain Adaptive Faster R-CNN for Object Detection in the Wild\cite{Chen2018}.}
	\begin{center}
		\label{Faster_rcnn_DA}
	\end{center}
\end{figure}
\FloatBarrier

Here, to maximize the domain classification losses $L_{inst}$ and $L_{img}$ , they are calculated as shown in Equation \ref{faster_DA_losses}. 

\begin{equation}
\label{faster_DA_losses} 
\begin{gathered}
\mathcal{L}_{i m g}=-\sum_{i, u, v}\left[D_{i} \log p_{i}^{(u, v)}+\left(1-D_{i}\right) \log \left(1-p_{i}^{(u, v)}\right)\right] \\ 
\mathcal{L}_{i n s} = -\sum_{i, j}\left[D_{i} \log p_{i, j}+\left(1-D_{i}\right) \log \left(1-p_{i, j}\right)\right]
\end{gathered}
\cite{Chen2018}, 
\end{equation}
where $D_i=0$ is a ground-truth label of the sample image $i$ that originates from  the source domain and $D_i=1$ originates from the target domain; $p_{i,j}$ is a prediction output of the instance-level classifier for $j$-th proposed region of the sample image $i$; $p_{i}^{(u, v)}$ is a prediction output of the image-level domain classifier for the feature map region $(u,v)$ of a sample image $i$. 
Finally, two domain classifiers are regularized using consistency loss $\lambda \mathcal{L}_{\text {consistency }}$ as shown in Equation \ref{faster_DA_losses_consist}.  

\begin{equation}
\label{faster_DA_losses_consist} 
L_{\text {consistency }}=\sum_{i, j}\left\|\frac{1}{|N|} \sum_{u, v} p_{i}^{(u, v)}-p_{i, j}\right\|_{2}
\cite{Chen2018}, 
\end{equation}
Due to the fact that the image-level domain classifier produces multiple $N$ predictions per image, first the average of them is calculated, and then the Euclidean (or L2 norm) distance is measured between the predictions of the image- and instance-level domain classifiers \cite{Chen2018}. 

The final objective of the network is then defined by minimizing the detection loss, while maximizing the image-level $\mathcal{L}_{img}$ and instance-level $\mathcal{L}_{inst}$ domain classification losses, while also minimizing the consistency loss $\lambda \mathcal{L}_{\text {consistency }}$ between two classifiers \cite{Chen2018}.

Another most recent state-of-the-art study made by Rezaeianaran et al. \cite{Rezaeianaran2021} proposed different approach that compares the adversarial training to contrastive learning. Similarly to Chen et al. \cite{Chen2018}, the network leverages a Faster-RCNN detector with instance- and image-level losses. The key difference is in the way the latter two losses are calculated. Rezaeianaran et al. attempted to push the features closer if they represent the same class and push them apart otherwise by utilizing max-margin contrastive loss. The margin here denotes how far the features can be in order to be considered the same class. Constrastive loss took the form of Equation \ref{contrastiveL}:

\begin{equation}
\mathcal{L}_{C L}=\sum_{i}^{C}\left[\left\|F_{S}^{i}-F_{T}^{i}\right\|_{2}^{2}+\sum_{j, j \neq i}^{C} \max \left\{0, m-\left\|F_{S}^{i}-F_{T}^{j}\right\|_{2}^{2}\right\}\right]
\label{contrastiveL} 
\cite{Rezaeianaran2021} 
\end{equation}

Additionally, as the the paper tried to solve UDA, no labels were available from the target domain and, therefore, pseudo-labelling was used to calculate the contrastive loss. \todo{This paper review should be rewritten} 


\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./VisGa.png}
	\end{center}
	\caption{Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection, adapted from \cite{Rezaeianaran2021}.}
	\begin{center}
		\label{visga}
	\end{center}
\end{figure}
\FloatBarrier


\subsubsection{Pseudo-labeling based methods}

Another relatively straightforward method to solve an object detection problem can be done by using pseudo-labels. A naive approach to pseudo-labelling is to first train the source dataset $\mathcal{D}_{\mathcal{S}}=\left\{\mathcal{X}_{\mathcal{S}}^{i}, \mathcal{Y}_{\mathcal{S}}^{i}\right\}_{i=1}^{\mathcal{N}_{\mathcal{S}}}$ and later run inference to obtain pseudo-labels on the target dataset $\mathcal{D}_{\mathcal{T}}=\left\{\mathcal{X}_{\mathcal{T}}^{j}\right\}_{j=1}^{\mathcal{N}_{\mathcal{T}}}$. The resulted labels will then form a new dataset $\dot{\mathcal{D}_{\mathcal{T}}}=\left\{\mathcal{X}_{\mathcal{T}}^{j}, \mathcal{Y}_{\mathcal{T}}^{j}\right\}_{j=1}^{\mathcal{N}_{\mathcal{T}}}$ \cite{Oza2021}. However, the results obtained will naturally be noisy and of poor quality. Khodabandeh et al. proposed a three-phase training process illustrated in Figure \ref{robust} below. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./robust.png}
	\end{center}
	\caption{A Robust Learning Approach to Domain Adaptive Object Detection \cite{Khodabandeh2019}.}
	\begin{center}
		\label{robust}
	\end{center}
\end{figure}
\FloatBarrier

In the first stage, the network is treated as a typical Faster-RCNN network that is trained on the source dataset $\mathcal{D_S}$. Next, the pseudo-labels are generated as explained earlier. In the following stage, the proposed regions are fed into a pretrained image classifier, which allows to refine the model.

For the process of refinement, Khodabandeh et al. proposed to use the Kullback-Leibler divergence and defined the optimization objective for classification as follows: 

\begin{equation}
\min _{q} \operatorname{KL}\left(q\left(y_{c}\right) \| p_{c l s}\left(y_{c} \mid \boldsymbol{x}, \tilde{\boldsymbol{y}}_{l}\right)\right)+\alpha \operatorname{KL}\left(q\left(y_{c}\right)|| p_{i m g}\left(y_{c} \mid \boldsymbol{x}, \tilde{\boldsymbol{y}}_{l}\right)\right)
\label{KL_DV} 
\cite{Khodabandeh2019},
\end{equation}

where $y_c$ is the class label, $y_l$ is the bounding box location, $\alpha$ is a trade-off between two terms,  $p_{img}$ is the classification prediction of the image classification model and $p_{cls}$ is the classification prediction of the Faster-RCNN detector model. The goal of the refining process is to find a distribution $q(y_c)$ that is close to the models of $p_{cls}$ and $p_{img}$. The process is relatively similar for the bounding box refinement and the reader is advised to consult the original paper for more details \cite{Khodabandeh2019}.  

The third stage of the process finalizes the strategy by retraining the final network with the labeled ground truth data from $\mathcal{D_S}$ and the refined pseudo-labels from $\dot{\mathcal{D_T}}$. 


\subsubsection{Image-to-Image translation}
\label{imagetoimage} 
Another category that Oza et al.\cite{Oza2021} outlined is image-to-image translation for UDA. Instead of trying to align features, this group of methods essentially attempt to pull the domains together first. Hsu et al. \cite{Hsu2019} has suggested a Cycle-Generative Adversarial Network(GAN) \cite{Zhu2017} based approach to transform images from the target domain into the source domain alike images. Figure \ref{gan} represents the complete network proposed by Hsu et al. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./GAN.png}
	\end{center}
	\caption{Progressive Domain Adaptation for Object Detection and CycleGAN, adapted from \cite{Hsu2019}.}
	\begin{center}
		\label{gan}
	\end{center}
\end{figure}
\FloatBarrier

The generator part of the Cycle-GAN in Figure \ref{gan}(a) creates intermediate domain from images in the source domain. The source images are then passed together with the labels to the Faster-RCNN network. The network is then tries to adapt the source domain to the sythetic domain. To further minimize the domain shift, adversarial learning techniques are used, such as a combination of a GRL and a domain classifier \cite{Hsu2019}.


In Figure \ref{gan}(b), the synthetic source-alike images are then passed together with the inherited source labels back to the adaptation network to align the features with the target domain. In the second stage of the training process, Hsu et al. proposed to utilize the weights $w$ from the discriminator of the Cycle-GAN, which is additionally trained to differentiate between source and target domains. Ultimately, such approach allowed to amplify the importance of the synthetic samples that are closer to the target domain. The weighted loss from the discriminator was then summed with the detection loss and the adversarial loss to finally adapt the synthetic domain to target \cite{Hsu2019}. 

\subsubsection{Domain randomization}

Oza et al. \cite{Oza2021} argues that often the accuracy of image-to-image translation methods is questionable as the domain shift between the synthetic and source domains still exists. Slightly different approach has been offered by Kim et al. \cite{Kim2019}. Instead of trying to pull two given domain distributions closer, they proposed a domain randomization technique, which generally attempts to generate a brand new domain that includes the same image in different style. This in practice allows the detector network to recognize features that are domain invariant and remove the domain bias. 

Kim et al. proposed a detector network that is based on Faster-RCNN with two additional components. The first component is a domain diversification module. Similarly to the method proposed by Hsu et al. \cite{Hsu2019}, the module leverages a Cycle-GAN \cite{Zhu2017}. However, the diversification module is used to generate images in a fixed set of new domains rather than to match the source dataset with target.  The second component is a multi-domain discriminator, which is essentially an adversarial feature learning approach, but instead of trying to confuse the detector in a binary set of domains, it tries to learn domain invariant features of multiple additional domains. The architecture of such domain randomization method is presented in Figure \ref{diversify}.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./diversify.png}
	\end{center}
	\caption{Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection, adapted from \cite{Kim2019}.}
	\begin{center}
		\label{diversify}
	\end{center}
\end{figure}
\FloatBarrier

During Cycle-GAN image generation, two additional loss terms limited the  randomization of an image - color preservation and reconstruction constraints. This was done in order to preserve features of the original image as it would otherwise affect the model negatively \cite{Oza2021}. In the original experiments Kim et al. considered three additional domains: a color preserved domain, a reconstructed domain, and a domain that combines both. Images from all the domains are then fed into the detector along with the inherited source labels   to train a domain-invarian network with help of a multi-class domain classifier and the resulted model was used to verify performance on the target dataset. 

\subsubsection{Mean Teacher and Graph Reasoning}
\label{mean_teacher} 
Another common approach utilized in domain adaptation and transfer learning in general is mean teacher training. A typical mean teacher setup consists of two equivalent models. However, these models are trained using two separate strategies to adapt the detector network. On the other hand, graph reasoning based approaches of UDA have been gaining popularity not only in image classification problems, but also in object detectors. One potential reason for this is because graph models are easily applicable with other adaptation methodologies \cite{Oza2021}. Cai et al. \cite{Cai2019} proposed an architecture that combines both mean teacher and graph reasoning techniques in one solution and such architecture is presented in Figure \ref{graph_MT}. 
 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./graph_teacher.png}
	\end{center}
	\caption{Exploring Object Relation in Mean Teacher for Cross-Domain Detection\cite{Cai2019}.}
	\begin{center}
		\label{graph_MT}
	\end{center}
\end{figure}
\FloatBarrier

The term "graphs" represents a complex data structure that contains multiple nodes or vertices connected to each other via edges. In case of an image as a graph, each pixel can be considered a node, which is linked to all of its neighbours. 
The setup developed by Cai et al. \cite{Cai2019} introduces a student-teacher framework based on Faster-RCNN that verifies the consistency of the graphs at three different levels using regional-level consistency, intra-graph consistency and inter-graph consistency. 

The training pipeline is split into two parts. Images from the source domain are trained in a locked student environment in a supervised manner. Images from the target model, on the other hand, are augmented with in two iterations. First, the images are randomly cropped, padded or flipped. These images are passed through the pretrained supervised student model that generates predictions. Meanwhile, the original target images are augmented with color jittering or PCA noise, and this set is send to the teacher model. The teacher model also produces predictions, which are then compared with the set of predictions from the first round of augmented images by utilizing region-level consistency. Essentially, the region-level consistency is calculated as MSE of the region-level prediction error of both the student and the teacher. 

Inter-graph level consistency is used to verify the quality of the two graphs produced by the teacher and student models. It is calculated by means of cosine similarity between the graph representations of the proposed regions. 

Finally, intra-level consistency is then calculated to measure the quality of predictions within the same class of the student model. However, since target domain has no labels included in the UDA setup, the closest prediction $\operatorname{argmin} (labels)$ is used to produce pseudo-labels. The intra-level consistency loss is then calculated for any two instances of the same class in one graph. For more detailed calculation of the loss terms the readers are referred to the original paper by Cai et al. \cite{Cai2019}.

A purely mean-teacher-based semi-supervised approach has been proposed by Liu et al. \cite{Liu2021} As it can be seen from the overview illustrated in Figure \ref{unbiased}, it consists of two sequential stages. During the burn-in stage, Faster-RCNN detector is trained on the labeled data as normal. Next, the training pipeline is split into two equivalent Faster-RCNN-based detectors. The teacher model is supplied with weakly-augmented data. On the other hand, strongly-augmented images are fed into the student model. According to Liu et al., the main reason for that was because while strong augmentation is needed to improve performance of the model, the weaker augmentations were still needed in the teacher model to generate reliable pseudo-labels. These pseudo-labels are in turn used in the student model.   
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./unbiased.png}
	\end{center}
	\caption{Unbiased Teacher for Semi-Supervised Object Detection\cite{Liu2021}.}
	\begin{center}
		\label{unbiased}
	\end{center}
\end{figure}
\FloatBarrier

In order to achieve higher accuracy of pseudo-labels, the unbiased teacher model includes an Exponential Moving Average(EMA) module, along with a few other techniques \cite{Liu2021}. EMA attempts to emphasize the most recent data by granting it a higher weight. The EMA in Unbiased teacher is defined as follows: 
\begin{equation}
\theta_{t}^{i}=\hat{\theta}-\gamma \sum_{j=1}^{i-1}\left(1-\alpha^{-j+(i-1)}\right) \frac{\partial\left(\mathcal{L}_{\text {sup }}+\boldsymbol{\lambda}_{u} \mathcal{L}_{u n s u p}\right)}{\partial \theta_{s}^{j}}
\label{EMA}
\cite{Liu2021},
\end{equation}

where $\hat{\theta}$ is the initial (burn-in stage) model weight, $\theta_{t}^{i}$ is the weight of the teacher model, $\theta_{s}^{j}$ is the weight of the student model  at $i$-th and $j$-th iterations respectively. The weight of EMA in the training process is defined by $\alpha$ and $\gamma$ is the learning rate(LR) of the ensembled model \cite{Liu2021}.

Liu et al. also discusses the class imbalance problem that often causes the detector to learn underrepresented classes poorly \cite{Liu2021}. In order to solve this problem, Liu et al. propose to make use of the multi-focal loss \cite{Lin2017}, which attempts to put more weight on the samples with lower confidence, unlike a generic cross-entropy loss that treats all samples equally. 

However, this method only solves a semi-supervised object detection without addressing the domain shift issue. Expanding the unbiased teacher method, Li et al. \cite{Li2021} proposed to utilize mean teacher training in a domain adaptation setup. In addition to the original ensembled network proposed by Liu et al.  \cite{Liu2021}, adaptive teacher architecture employs adversarial feature learning techniques to adapt the target domain to the source. A typical domain adaptation network, which includes a GRL and a domain classifier, is appended to the backbone feature extractor of the student model. The complete architecture is displayed in Figure \ref{adapt_teacher}. 
 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./adapt_teacher.png}
	\end{center}
	\caption{Cross-Domain Adaptive Teacher for Object Detection
\cite{Li2021}.}
	\begin{center}
		\label{adapt_teacher}
	\end{center}
\end{figure}
\FloatBarrier

Liu et al. \cite{Liu2021} specified that the teacher model is supplied with weakly-augmented target images, while the student model uses both strongly-augmented source and target images. Weak augmentations include cropping and flipping the image horizontally, and strong augmentations included grayscaling, color jittering, Gaussian blurring and cutting out patches. The same strategy was employed in this adaptive teacher method. The augmentations used in these works  \cite{Li2021, Liu2021} are presented in Figure \ref{augmentations}. 
\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=10cm]{./augmentations.png}
	\end{center}
	\caption{Augmentations used in Unbiased teacher (adapted from the official  Pytorch documentation \cite{pytorch})}
	\begin{center}
		\label{augmentations}
	\end{center}
\end{figure}


Although all of the methods discussed in this chapter are based on the two-stage object detector Faster-RCNN, fewer papers also addressed a single-stage object detection in a domain adaptation setup, such as UDA for YOLO \cite{Hnewa2021}, \cite{Zhang2021a}, UDA for FCOS \cite{fcos1} and UDA for DETR \cite{Zhang2021b}, \cite{Vidit2021}. However, their review will be omitted with the grounds for it given in the \nameref{Methodology} section. 

\subsection{Continual learning}
\label{cont_learning} 
While transfer learning attempts to apply knowledge collected from one domain to another, lifelong learning offers adaptive algorithms, which  would accept a continuous stream of information that becomes available over time \cite{Parisi2018}. In case of the simple object detection problem, continual learning can be applied to a new task, such as to learn new classes of objects that are supplied progressively over time. This approach is potentially useful due to the scalability benefits it brings, since retraining the entire model every time a new object arrives to the database is computationally expensive. 

Similarly to the human brain, ANNs in object detectors tend to forget old knowledge learned as the memory gets overwritten with fresh data. This phenomenon in continual learning is commonly addressed as "catastrophic forgetting" \cite{Parisi2018}. Parisi et al. summarizes the up-to-date methods of continual learning that are effective against catastrophic forgetting into three categories: retraining with regularization, selective training with network expansion and retraining selective network with expansion. These methods are illustrated in Figure \ref{continual}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./continual.png}
	\end{center}
	\caption{Continual learning approaches
\cite{Parisi2018}.}
	\begin{center}
		\label{continual}
	\end{center}
\end{figure}
\FloatBarrier

The methods proposed in Figure \ref{continual}(a) attempt to solve catastrophic forgetting by means of regularization. In these group of methods, the algorithm seeks to penalize the modifications in the model of the originally trained task. A simplistic approach of such method method was presented by Razavian et al.  \cite{Razavian2014}, where the gradients calculation for the parameters of the original is disabled completely. Slightly modified version of this method was also proposed by Donahue et al. \cite{Donahue2013}, where the LR was decreased to minimize the parameter update instead of blocking the update alltogether. 


In simplistic terms, the method of training with expansion denotes retraning the original network with an additional number of layers appended. A typical example of such a method was proposed by Rusu et al. \cite{Rusu2016}, where the network was trained on the initial task. As $N$ new tasks were added, $N$ number of layers were appended to the network, as shown in Figure \ref{continual}(b). The parameters of the original network were left unchanged, and only the additional connections were trained. Although the disadvantage of this method was that the network complexity would grow linearly over time as new tasks are added, the experiments delivered sufficient performance. 

The method proposed by Yoon et al. \cite{Yoon2017} falls into the third category, which is illustrated in Figure \ref{continual}(c). The concept is fundamentally similar to the one suggested by Rusu et al. \cite{Rusu2016}, with the exception that their model selectively retrains the network from the original task. Connecting the neurons sparsely reduced the computational overhead, as well as allowed the network to retain the previously learned tasks. Parisi et al. have collected many other promising contunual learning methods and the reader is advised to refer to the original paper.   

In the context of the notation defined earlier in the \nameref{transferLearning} subsection, continual learning can be expressed as training on the task $\mathcal{T}_n$ given the task $\mathcal{T}_{n-1}$ within the same domain or a set of domains $\mathcal{D}_{n} = \mathcal{D}_{n-1}$.   

\todo{perhaps add the final CL graph from Parisi2018} 


\clearpage 

\section{Research Methodology}
\label{Methodology}

In this section, an in-depth study will be conducted to solve the challenges outlined in the \nameref{objective}. The section will first discuss the process of dataset selection. Next, several object detection candidates will be analyzed.  The selected object detection method will be examined in multiple domain-adaptive setups. Upon analyzing the exisiting DA methods, one candidate will be studied in greater detail to propose a novel architecture. Finally, continual learning will be investigated in the selected cross-domain object detection model. 


\subsection{Dataset}
\label{datasets} 
Initially, one of the objectives of this thesis was to implement an equipment identification system using images of the installed equipment and images of the 3D models to expand the dataset. Images of the 3D equipment parts could potentially be the main source of the dataset and they could be obtained from the 3D model of the gold refining plant of the customer of Metso Outotec. The entire plant's model is stored in Autodesk Navisworks in \texttt{.nwd} format and contains thousands of different equipment parts. It was discovered that Navisworks offers a possibility of rendering both images and videos of the desired equipment. However, in order to make the process of data collection scalable, it was proposed to use Navisworks API to automate the process. Following the documentation of the API  \cite{navisworks}, a simple script with an extra toolbox were added Navisworks to filter out the required model, render and export its images in \texttt{.jpg} format. The results are presented in Figure \ref{navisworks} below.

\begin{figure}[htb]
    \centering
    \subfloat{{\includegraphics[width=6cm]{./toolbox.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=6cm]{./model.png} }}%
    \caption{Example of the rendered image of an arbitrary model}
    \label{navisworks}%
\end{figure}
\FloatBarrier

Upon a quick research, it was discovered that it is possible to automatically rotate around the selected object in order to render images from all sides of the equipment as it is important to have variety while accumulating a dataset. However, unlike the dataset with rendered images, a dataset with images of real equipment turned out to be troublesome due to confidentiality and accessibility issues. Moreover, considering that rendered images would require further processing and labeling, it was decided to switch to an open-source dataset for the purpose of this project. 

Previously, in the \nameref{neural_nets} section, few datasets, such as ImageNet \cite{Russakovsky2014}, PASCAL VOC \cite{Everingham10} and COCO \cite{Lin2014} were briefly mentioned. These datasets are universally used in image classification and object detection problems. However, in order to leave room for further research as well as to align with the objectives of the thesis, the dataset should ideally consist of industrial equipment and include corresponding 3D models for each object. Datasets such as ImageNet, PASCAL VOC and COCO typically contain generic objects that people face in everyday life. 

Ultimately, it was concluded that the dataset named Texture-LESS(T-LESS)  \cite{hodan2017tless} meets the requirements. It consists of nearly 39 000 training and 10 000 testing images of thirty industry-relevant objects. The training subset consists of rendered images in a simulated environment, while the test subset is taken in real-life conditions.  Different objects in the dataset are often distantly similar to each other, which makes the task slightly more challenging. Finally, the dataset also includes Computer-Aided-Design(CAD) \texttt{.ply} files with 3D models of the objects, which can then be easily converted into any other required CAD format. Unfortunately, the dataset is originally meant for 6D-pose estimation \cite{hodan2017tless} as a part of the Benchmark for Pose Estimation(BOP) challenge \cite{hodan2018bop}. As a result, the format of the dataset is derived from the format defined by BOP  \cite{hodan2018bop_format}. Therefore, a script was developed to convert it into a format commonly used by the state-of-the-art object detection frameworks, as well as to remove redundant information that is only applicable to pose estimation tasks. Initially, the dataset was converted to mimic YOLO \cite{Redmon2015a} format, but later it was switched to PASCAL VOC due to its better flexibility in two-stage object detectors. An example of the annotated T-LESS image from a real setup is illustrated in Figure \ref{tless_real_example}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=10cm]{./tless_real_annotated.png}
	\end{center}
	\caption{T-LESS real setup, labelled
\cite{hodan2017tless}.}
	\begin{center}
		\label{tless_real_example}
	\end{center}
\end{figure}
\FloatBarrier

The names of the object classes presented in T-LESS, although undisclosed, are irrelevant to this thesis. Therefore, here and in the subsequent sections, when referred to individually, the classes will be named as "Model $N$". Additionally, in the experiments, the datasets with images from the simulator and the images from the real environment setup will be mainly referred to as "source" and "target" datasets, respectively. The images from the source dataset are saved in \texttt{.jpg} format, while the real images are stored as \texttt{.png} files, similarly as in the original T-LESS \cite{hodan2017tless}. Furthermore, for this work, the source dataset will be split in 85\%/15\%  proportion into the training and testing subsets. The distribution of the classes in the source dataset is presented in Figure \ref{tless_distribution_rend}.  

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./rendered_distribution.png}
	\end{center}
	\caption{Distribution of the classes in the rendered subset of T-LESS dataset. 42 500 training images and 7 500 testing images. Total number of object instances: 720443}
	\begin{center}
		\label{tless_distribution_rend}
	\end{center}
\end{figure}
\FloatBarrier

On the countrary, real images are only used for validation and their distribution by class is shown in Figure \ref{tless_distribution_real}. The reasons for this will be discussed in later sections. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./real_distribution.png}
	\end{center}
	\caption{Distribution of the classes in the real subset of T-LESS dataset. Total number of object instances: 10362}
	\begin{center}
		\label{tless_distribution_real}
	\end{center}
\end{figure}
\FloatBarrier

\subsection{Preliminary experiments}
In the long term, the idea behind the project is to implement a scalable system that would identify real-life images of the equipment given the 3D images from the simulator. Therefore, as a default setup it was decided to use the rendered T-LESS-based dataset for training, and the real T-LESS-based dataset for evaluation. Naturally, in object detection problems, choosing the right detector is just as important as preparing the dataset. For the initial experiments, Faster-RCNN  \cite{Girshick2015} model was tested.

Additionally, YOLOv3 \cite{Redmon2018a} was considered. However, unlike Faster-RCNN, YOLO has not been as popular, according to Figure \ref{popularity}. This was partially due to flexibility of Faster-RCNN when it comes to replacing different components of the network in order to improve the results. Although YOLO proved to be significantly faster than Faster-RCNN as discussed earlier in the \nameref{yolo_section} section, in this thesis, the flexibility of the network and its accuracy are treated  as higher priority tasks. For these reasons, Faster-RCNN will be used in all further experiments.  
   

\subsubsection{Metrics}
\label{metrics_section} 

As for the detection evaluation, here and in the further experiments and in order to match the selected dataset format, the metrics were employed in accordance with the mean average precision metric(mAP) of the PASCAL VOC \cite{Everingham10} challenge. In order to measure the mAP, first the confusion matrix should be calculated. Confusion matrix is a generic performance measurement tool in ML. According to PASCAL VOC, precision and recall are the most important terms in object detection tasks that can be extracted from the confusion matrix and their equations are shown in Table \ref{confusion}.

\begin{table}[htb]
	\captionof{table}{Definition of confusion matrix and some of its terms, adapted from \cite{mAp_blog}.}
	\begin{center}
		\includegraphics[width=14cm]{./confusion.png}
	\end{center}
	\begin{center}
		\label{confusion}
	\end{center}
\end{table}
\FloatBarrier


To calculate precision and recall, it is important to define how the True Positive(TP) term is calculated. This is essentially done by measuring the  Intersection-over-Union(IoU) metric. As the model proposes an anchor box, the classifier ouputs a confidence score. The confidence score is a probability of the box to contain an object. The region is then additionally compared against the ground truth bounding box labels using the Equation \ref{iou}. Finally, if the IoU ratio, also known as Jaccard distance, is above the pre-defined threshold, the predicted object is considered to be a TP. In PASCAL VOC evaluation, the default threshold is 0.5 \cite{mAp_blog} and only predictions with the highest confidence score count. 

The prediction is labeled as False Positive(FP) when either the predicted class is wrong or its IoU is below the threshold. Otherwise, if the confidence score of the proposed region is lower than the threshold, the value is labeled as False Negative(FN). Next, precision and recall values are calculated as specified in Figure \ref{confusion}.

\begin{equation}
\operatorname{IoU}\left(\text { Area }_{\text {Prediction }}, \text { Area }_{\text {Ground truth }}\right)=\frac{\mid \text { Area }_{\text {Prediction }} \cap \text { Area }_{\text {Ground truth }} \mid}{\mid \text { Area }_{\text {Prediction }} \cup \text { Area }_{\text {Ground truth }} \mid}
\label{iou} 
\cite{mAp_blog} 
\end{equation}

The area under the Precision-to-Recall curve is known as Average Precision(AP). The pairs of precision and recall are recorded at multiple confidence scores and compared against each other in the Precision-to-Recall curve. In PASCAL VOC 2010-2012 \cite{Everingham10}, the area under the curve is smoothed first, and then the rectangles below the interpolated curve are summed as shown in Figure \ref{AUC}. The interpolation $p_{\text {interp }}$ of the curve $p$ is performed using Equation \ref{interp}, where $r$ is a recall level. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=12cm]{./AUC.png}
	\end{center}
	\caption{Average precision curve \cite{mAp_blog}.}
	\begin{center}
		\label{AUC}
	\end{center}
\end{figure}
\FloatBarrier


\begin{equation}
p_{\text {interp }}(r)=\max _{r^{\prime} \geq r} p\left(r^{\prime}\right)
\label{interp}
\cite{mAp_blog}  
\end{equation}

In the subsequent sections, AP50 and AP75 denote average precision at IoU of 0.5 and 0.75, respectively, as defined by COCO \cite{Lin2014}. According to PASCAL VOC \cite{Everingham10} evaluation methods, AP stands for average precision at all confidence intervals, unlike COCO \cite{Lin2014} and earlier iterations of PASCAL VOC, where AP was only calculated at 11 equally spaced fixed intervals. Finally, in the following sections, the terms mAP and AP will be used interchangeably. 


\subsubsection{Naive approach}
\label{naive} 
After directly applying Faster-RCNN detector\cite{ima} in detectron2 \cite{wu2019detectron2} framework to the prepared dataset and evaluating it using the metrics presented earlier, it quickly became evident that the solution has to be more complex in order to solve the challenges outlined in the \nameref{objective}. 

For the initial experiments, a Faster-RCNN network with a ResNet backbone was used. The ResNet architecture had 50 layers. The training has been conducted in three stages. First, the model was trained on the source dataset. Next, the trained model was evaluated on the real images of the same classes. Finally, identical model has been trained on the target dataset for comparison. After training the model for a short duration, corresponding to 20 000 iterations and the base LR of 0.00125, the following results were obtained: 

\begin{table}[htb]
	\captionof{table}{Experiments with a simple Faster-RCNN model. }
	\begin{center}
		\includegraphics[width=14cm]{./initialExp.png}
	\end{center}
	\begin{center}
		\label{faster_init}
	\end{center}
\end{table}
\FloatBarrier

As it can be concluded, there is a significant performance drop when the environment is slighly changed. This is essentially a result of two related problems combined: overfitting and domain shift. Naturally, two datasets are originated from different environments and not only their background, but also their lighting conditions, positioning and textures deviate from one another. 

\subsubsection{Experiments with existing standard domain adaptive methods}
To overcome the domain shift phenomenon, it was proposed to experiment with domain adaptation applications. For these experiments, two existing open-source methods were suggested. 

In the initial study, a model based on decoupled adaptation for cross-domain object detection \cite{Jiang2021} was tested. The model introduced by Jiang et al. essentially proposes an \nameref{adv_approach} approach, where a GRL is applied to the classifier and the box regressor in a decoupled way, i.e. the problem is split into two subproblems to avoid them from interfering with each other. According to Jiang et al., this could improve the discriminability of the detector. Readers can refer to the original paper \cite{Jiang2021} for more information. The results of the experiments are outlined in the Table \ref{dadapt}.

\begin{table}[htb]
	\captionof{table}{Results of the experiments with a D-Adapt based method.}
	\begin{center}
		\includegraphics[width=14cm]{./dadapt.png}
	\end{center}
	\begin{center}
		\label{dadapt}
	\end{center}
\end{table}
\FloatBarrier

First, a model was trained on the source dataset for 4 hours. Similarly to the \nameref{naive}, the performance drops dramatically when testing the model on the target dataset. However, after running the adaptation network for another 4 hours, the results have improved by a considerable margin. The final result on the combined dataset is $AP = \frac{57.69+36.85}{2} = 47.27$. Although the result is much better than the one without any adaptation with $AP = 9.0097$, this performance seems to be rather insufficient. 

Recently, many open-source solutions proposed \cite{Inoue_2018_CVPR, Chen2020, Arruda2019} an \nameref{imagetoimage} approach in a cross-domain object detection setup. Following Zhu et al., Cycle-GAN \cite{Zhu2017} was used in an attempt to generate an intermediate domain. The results obtained after training the model for 44 epochs(= 36 hours) are compiled in Table \ref{cyclegan}.  

\begin{table}[htb]
	\captionof{table}{Results of the experiments with Cycle-GAN}
	\begin{center}
		\includegraphics[width=16cm]{./cyclegan.png}
	\end{center}
	\begin{center}
		\label{cyclegan}
	\end{center}
\end{table}
\FloatBarrier

As it can be noted, even after an extensive training with a massive dataset of nearly 50 000 images, Cycle-GAN still produced fairly low-quality results, especially for the real-alike images. This problem has also been acknowledged by Zhu et al. as a limitations of Cycle-GAN \cite{Zhu2017}, where differences in the distribution characteristics of the training dataset caused comparable artifacts. Due to the low performance on the T-LESS dataset and long training time, this method was excluded from further tests. 


\subsection{Experiments with an ensembled method}
\label{ensemExp} 
For the next set of experiments, this thesis proposes an ensembled setup. An ensembled setup is a combination of multiple algorithms that leverage their benefits to produce a superior result. In regards to domain adaptive object detection, such setups were summarized earlier in the \nameref{mean_teacher} section. As a base study for this thesis, the adaptive teacher \cite{Li2021}  framework is studied extensively. Unlike the  previous architectures, mean teacher training frameworks implement multiple adaptation strategies. In case of the adaptive teacher method, Li et al. proposed to use both adversarial and pseudo-labeling techniques. To validate the suitability of this architecture, the codebase has been modified to accept the prepared T-LESS dataset and was trained as it is. Figure \ref{adapt_experiment1} illustrates the results when visualizing the trained model. Similarly, to the previous experiments, the model was based on Faster-RCNN \cite{ima} and a ResNet \cite{He2015} backbone network with 101 layers, which was pretrained on the ImageNet \cite{Russakovsky2014}dataset. Although traditional ML and DL algorithms utilize the term epoch, which stands for one forward and one backward passes of the loop for the entire set of the training samples, detectron2 \cite{wu2019detectron2} uses iterations instead. Iterations denote a number of times to complete the forward and backward passes for a batch of training samples, where a batch is a small subset of data. 

\begin{figure}[htb]
    \centering
    \subfloat[Visualization on the source dataset]{{\includegraphics[width=6cm]{./experiments_original_1.png} }}%
    \qquad
    \subfloat[Visualization on the target dataset]{{\includegraphics[width=6cm]{./experiments_original_2.png} }}%
    \caption{Results of the experiments with Adaptive Teacher as it is}
    \label{adapt_experiment1}%
\end{figure}
\FloatBarrier

Expectedly, the model performs poorly on the custom dataset without tweaking any hyperparameters. Although the performance on the source dataset is more than satisfactory (Figure \ref{adapt_experiment1}(a)), random artifacts take place in the form of noisy bounding boxes, as can be seen in Figure \ref{adapt_experiment1}(b). For this round of experiments, the number of iterations was set as 360 000. This value is potentially reduntant for a considerably large dataset such as T-LESS.  However, in order to explore the limits of the network it will be tested nevertheless.  
\todo{perhaps convert iterations to epochs} 

To overcome this issue, two steps have been carried out. First, the LR scheduler has been adjusted in the original codebase. In the original implementation, the Adaptive teacher algorithm used a warmup two stage multi-step scheduler. This scheduler initializes the LR in two stages, where it first gradually increases for a number of warmup iterations $\texttt{WARMUP\textunderscore ITERS} = 1000$, until it reaches the fixed $\texttt{BASE\textunderscore LR} = 0.001$.  Instead, this thesis suggests to use the multi-stage scheduler with a cosine annealing, as suggested by Loshchilov et al. \cite{Loshchilov2016}. In these experiments, the learning rate first slowly ascends to the $\texttt{BASE\textunderscore LR} = 0.001$. However, as the ceiling is reached, it starts to decay to zero by the time of the last iteration $\texttt{MAX\textunderscore ITERS}$ is reached. This in theory allows to protect the scheduler from overshooting as it approaches the global minimum. Although Loshchilov et al. additionally proposes resetting the scheduler after it reaches zero, in this thesis such implementation is neglected for the time being. Both schedulers are presented in Figure \ref{lr}.  

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=10cm]{./lr.png}
	\end{center}
	\caption{LR scheduler, original (blue) and suggested (red)}
	\begin{center}
		\label{lr}
	\end{center}
\end{figure}
\FloatBarrier

Another issue that potentially caused the noisy predictions on the real images is overfitting. This might happen when, given the model complexity, the model is trained for an excessive amount of time. As a result, the model would only perform well on the training data. 
Therefore, in order to identify the suitable number of iterations required for training, an early stoppage algorithm was added. Instead of checking whether the loss keeps decreasing, the $AP50$ metrics is tracked for the validation set at every \texttt{EVAL\textunderscore PERIOD} interval. If the value does not improve for more than $\texttt{EVAL\textunderscore PERIOD} \times \texttt{PATIENCE}$ iterations, the best model is saved and the training is terminated. An example performance of these two suggestions combined are compared against the original model and are illustrated in Figure \ref{exp1}. 

As a result, the noisy bounding boxes have dissapeared and the combined AP50 on both dataset improved from 
\todo{make better plots and add AP comparison} 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=10cm]{./adapt_teacher_exp1.png}
	\end{center}
	\caption{AP50 performance with original configuration(red) against the performance of the suggested scheduler + early stoppage (blue)}
	\begin{center}
		\label{exp1}
	\end{center}
\end{figure}
\FloatBarrier


\subsection{Further experiments with a custom model}
\label{mainExperiments} 
In the subsequent sections, a novel architecture will be introduced. The effectiveness of the existing methods has been evaluated to address domain shift  between various open-source datasets. Most often domain adaptation is performed to transfer knowledge from one dataset for autonomous driving(Cityscapes) to another(Foggy-Cityscapes, Sim10k, KITTI), or to transfer knowledge from one dataset with general objects (PASCAL VOC) to another(Clipart, Watercolor, Clipart) \cite{Oza2021}.  

However, no comprehensive work have addressed the industrial cross-domain object detection. Additionally, motivated by works of Li et al. \cite{Li2021} and Xu et al. \cite{Xu2020}, this thesis assesses the practicality of the categorical regularization in the adaptive teacher framework. Unlike the original architecture of Adaptive teacher \cite{Li2021}, the model is aligned at two stages:  image-level and instance-level. Following Chen et al. \cite{Chen2018} and Xu et al. \cite{Xu2020}, a consistency loss term has been added to regularize both stages. The training process follows the principles outlined in the \nameref{ensemExp} section. The described architecture is presented in Figure \ref{mymodel}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./MyModel.png}
	\end{center}
	\caption{A proposed architecture for cross-domain object detection. Blue elements represent standard Faster-RCNN components, purple elements - domain adaptation components and yellow elements are Faster-RCNN modified components for continual learning, which will be discussed later.}
	\begin{center}
		\label{mymodel}
	\end{center}
\end{figure}
\FloatBarrier

Chen et al. \cite{Chen2018} was one of the first methods to use a two-level alignment, which was discussed earlier in the \nameref{adv_approach} section. Xu et al. \cite{Xu2020} expanded further on this method by adding a consistency regularizing term. Upon analyzing multiple papers, Xu et al. comments that using only image-level adaptation would result in the alignment of the non-transferrable background regions between the source and target domains, which would in turn result in poor accuracy in the target domain \cite{Xu2020}. Insance-level alignment would allow to reduce the distance between the local features, such as object appearance, size and viewpoint \cite{Chen2018}. On the other hand, it is not possible to omit image-level alignment as it is important to address adaptation of the global features of the image, such as image scales, illumination and style \cite{Chen2018}. 

Therefore, this setup employs a mean teacher based Faster-RCNN \cite{ima} setup with two domain adaptation networks in the student model. Similarly to the preceding architectures \cite{Li2021,Liu2021}, data augmentation is used to assist in the training process, For these experiments, random cropping was excluded from tests as it was negatively affecting the pseudo-labeling. Instead, a few other transformations such as histogram equalizing and sharpness adjustments, were added to strong augmentation. The complete list of used augmentations is shown in Figure \ref{newAugmentations}. It is believed that rotations and random affine transformations are not as important with the T-LESS dataset as the number of images taken in a rotational manner seems to be fairly sufficient.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./Tless_augm.png}
	\end{center}
    \caption{Augmentations used in the experiments.}
	\begin{center}
		\label{newAugmentations}
	\end{center}
\end{figure}
\FloatBarrier


Following Li et al. \cite{Li2021}, one adaptation module is attached to the output of a CNN backbone, which was chosen to be ResNet \cite{He2015}. Another domain adaptation network is attached to the output layers of Faster-RCNN. While the backbone generates image-level features, Faster-RCNN outputs the proposed foreground regions. Both modules utilize a standard GRL block \cite{Ganin2015} followed by a domain classifier.




In the experiments, the image-level domain classier was defined in accordance with the setup of Li et al. \cite{Li2021}. It has three convolutional layers with batch normalization and leaky-ReLUs followed by a convolutional classier layer. The instance-level classier resemble the architecture in \cite{Xu2020} and consists of two linear layers with leaky-ReLUs and dropout layers and finalized with a classier and a sigmoid. 

Naturally, both an image and a region proposed by Faster-RCNN should represent the same domain. Similarly to Xu et al., to minimize the difference between predictions of the domain classifiers, they are regularized using $\mathcal{L}_{\text {consist }}$ consistency loss. In these experiments, consistency loss is based on simple MSE loss between instance- and image-level domain classifier loss terms $\mathcal{L}_{\text {ins }}$ and $\mathcal{L}_{\text {img }}$.

The calculations of both $\mathcal{L}_{\text {ins }}$ and $\mathcal{L}_{\text {img}}$ follow the practices outlined in Equation \ref{faster_DA_losses}. Additionally,  the consistency loss $\mathcal{L}_{\text {consist }}$ follows the formula in Equation \ref{faster_DA_losses_consist}. However, instead of L2 norm, MSE is utilized.  

To trade-off between different loss parameters, the weights of the domain classification and consistency losses have been added to the original setup of the Adaptive teacher network. The complete formula for loss computation is displayed in Equation \ref{total_loss}. 

\begin{equation}
\mathcal{L}=\mathcal{L}_{\text {sup }}+\lambda_{\text {unsup }} \cdot \mathcal{L}_{\text {unsup }}+\lambda_{\text {img }} \cdot \mathcal{L}_{\text {img }}+\lambda_{\text {ins }} \cdot \mathcal{L}_{\text {ins }}+\lambda_{\text {consist }} \cdot \mathcal{L}_{\text {consist }},
\label{total_loss} 
\end{equation}

where $\mathcal{L}_{\text {sup }}$ is the standard Faster-RCNN detector loss in the student model; $\mathcal{L}_{\text {unsup }}$ is the unsupervised loss between pseudo-labels generated by the teacher model and supervised predictions and the $\lambda$-s are the corresponding hyper-parameters to regularize the weights of different loss terms. The $\mathcal{L}_{\text {sup }}$ is calculated in a similar manner as it was introduced earlier in Equation \ref{faster_rcnn_loss}. 
 
Pseudo-labels for the unlabeled target dataset are obtained identically as in \cite{Li2021}. The noisy pseudo-labels are filtered using a confidence threshold and a class-wise NMS \cite{Liu2021}. These pseudo-labels are considered as a ground-truth in the calculation of the unsupervised loss $\mathcal{L}_{\text {unsup }}$. It is essentially based on a standard Faster-RCNN loss. However, it does not include the bounding box regression loss, as it not possible to filter out the bounding boxes using only confidence threshold, which was specified by Liu et al. \cite{Liu2021}.  Instead, to improve the quality of pseudo-labels, EMA is applied to gradually move weights from the student to the teacher model as discussed in the  \nameref{mean_teacher} section, and it is the only place where updating the teacher model takes place \cite{Li2021}.  
 
Multiple experiments have been conducted with the architecture defined in Figure \ref{mymodel}. In these experiments, the detector head is a regular Faster-RCNN component. 
\todo{add plots} 
\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=16cm]{example-image} 
	\end{center}
    \caption{Augmentations used in the experiments.}
	\begin{center}
		\label{newAugmentations}
	\end{center}
\end{figure}
\FloatBarrier

In the following section, continually learning domain adaptation setup is introduced. 


\subsection{Experiments with continual learning}
\label{cont_learning_section} 
Although the pretrained model performs fairly well on the target dataset, it will not be able to predict unknown objects. In order to solve the scalability problem defined in the \nameref{objective}, the proposed architecture should be able to keep learning when new data is added. Often such problems are addressed by a special case of transfer learning known as "fine-tuning". A typical example of fine-tuning is done by freezing certain deeper layers and only retrain the top level components of the network. This procedure would enable the network to apply previously learned knowledge on an unknown yet similar problem, while significantly reducing the training time. In regards to the objectives of this thesis, it would allow to utilize the knowledge of the previously learned equipment parts to promptly retrain it and apply it to new classes of objects. 
In the first set of experiments, a naive fine-tuning approach without freezing was attempted. The model was trained based on the architecture, which was demonstated in the \nameref{mainExperiments}. The regular training stage included both source and target datasets that contained classes 1 to 20, while the fine-tuning stage only included the data for classes 21-30 and the pretrained weights file from the previous stage. After the model has been trained on the first 20 classes, the FC layers of the trained model have been removed and replaced with new FC layers that can produce predictions for 30 classes. Both models were trained for the same number of iterations. The results are presented in Table \ref{naive_finetuning}.

\begin{table}[htb]
\caption{The naive fine-tuning results}
\label{naive_finetuning}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|l|ll}
\cline{2-3}
                                         & Model trained on data from classes 1-20      & Model trained on data from classes 21-30   &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Base learning rate} & 0.001                              & 0.0005                           &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Outputs}            & Predictions for 20 classes         & Predictions for 30 classes       &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Max iterations}     & 35000                              & 35000                            &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Training time}      & about 8 hours                      & about 8 hours                    &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Data} &
  \begin{tabular}[c]{@{}l@{}}source (labeled) and target (unlabeled) \\ datasets limited to classes 1-20\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Saved weights(.pth) from the original model(1-20) \\ + datasets limited to classes 21-30\end{tabular} &
   &
   \\ \cline{1-3}
\multicolumn{1}{|l|}{Results}            & \textbf{AP} 58.1596, \textbf{AP50} 75.27, \textbf{AP75} 66.89 & \textbf{AP} 19.93, \textbf{AP50} 29.78, \textbf{AP75} 22.36 &  &  \\ \cline{1-3}
\end{tabular}%
}
\end{table}

As can be easily noted, the results dropped significantly even though the initial weights are supposed to give a decent advantage. This setup would not only have low performance on the new data, but also unable to classify and localize original dataset. A few important modifications were considered in the next experiment: 

\begin{enumerate}
\item In order to fine-tune the network successfully, a freezing process should be carried out on the deeper, more specifically, on the backbone layers of the network. Skipping this step is the main result of the performance drop in Table \ref{naive_finetuning}. By freezing deeper parts of the network, key trained features will be preserved, and the number of trainable parameters will be reduced, which will in turn speed up the training time. 
\item Too many unfamiliar cases will lead to a higher performance drop. Adding fewer additional classes in each subsequent task will help to maintain the performance drop at the lower level. This has been proven by experiments of Peng et al. \cite{Peng2020}. In other words, instead of retraining the original model with 10 more unknown classes, the model should be retrained with one or two at most. 
\item Additionally, it was noted that the weights file \texttt{.pth}, which was originally trained for the first $N$ classes, also contained the states of the optimizer, i.e. of the training process. This included the iteration number and the learning rate at that iteration. As a result, the training process is started from a much lower base LR, as it can be concluded from Figure \ref{lr}. Therefore, before proceeding, the optimizer states were removed from the dictionary of the weights file. 
\item Finally, instead of replacing the original FC layers alltogether, a new architecture is proposed, as illustrated in Figure \ref{continualModel}(b). 
\end{enumerate}

Following the studies presented in the \nameref{cont_learning} section, the detector head of the custom model is additionally modified to address a scalable training setup. The architecture presented in Figure \ref{continualModel}(b) replicates the method proposed by Rusu et al. \cite{Rusu2016}, but in a cross-domain mean teacher setup. An overview of this method was shown earlier in Figure \ref{mymodel}, where both detector heads of the student and the teacher networks are modified. In this experiment, the original detector head is frozen, which includes both the classifier and the box regressor. Freezing essentially prevents the weights from being modified by disabling the learning rate updates. Consequently, new output layers are attached to the network. These layers are responsible for the unknown $M$ number of classes and unlike the original detector, they are not frozen. The original detector head classifies an $N+1$ number of objects, which also includes the background. Therefore, to preserve data integrity, the appended detector head ignores the background class and only predicts $M$ new objects. The results of such experiment are presented below\todo{add plots}. 


To solve this problem\todo{write about the problem}, a new architecture has been introduced in Figure \ref{continualModel}(c). This model further combines the ideas proposed by Rusu et al. \cite{Rusu2016} and Donahue et al. \cite{Donahue2013}, which were introduced in the \nameref{cont_learning} section. Instead of completely freezing the original detector weights, the learning rate is reduced for the previously trained objects. The additional FC layers are trained with a standard LR.  According to Parisi et al. \cite{Parisi2018}, reduced learning rate will prevent the model from making significant changes in the original weights, while also learning new data. Meanwhile, the dynamically expanding architecture of Rusu et al. \cite{Rusu2016} will focus on fighting the catastrophic forgetting problem. Ultimately, in the proposed method the model aims to train network expansion along with regularization. 

The results of the proposed method are summarized below. \todo{add plots} 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=16cm]{./FC.png}
	\end{center}
	\caption{A proposed architecture for continual learning setup. Blue elements represent standard Faster-RCNN components, yellow elements are modified components for continual learning.}
	\begin{center}
		\label{continualModel}
	\end{center}
\end{figure}
\FloatBarrier



\subsection{Deployment}
In order to showcase the performance, a simple web app was prepared. The app was hosted on a local server and it leverages Flask API to attach the detector app to a user interface. The results are presented in Figure \ref{demo}. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=14cm]{./demo.png}
	\end{center}
	\caption{A screenshot of the simple web app. The image on the left-hand side is an uploaded target image, and the image on the right-hand side shows a prediction of the objects.}
	\begin{center}
		\label{demo}
	\end{center}
\end{figure}





\clearpage

\section{Analysis and Discussion} 

\subsection{Analysis of the approach}
\todo{talk about transfer learning inefficiency} 


\subsection{Directions for future work}
Inspired by Oza et al. \cite{Oza2021} and by the \nameref{objective}, this thesis studied multiple research directions in domain adaptation. Oza et al. summarized a few scenarios, where the cross-domain object detection was not researched as extensively. A few of these scenarios overlap with the thesis goals and these will be discussed in the following section. 

\subsubsection{Other applications and real world datasets}
As it was discussed in \nameref{mainExperiments} section, common DA approaches in object detection address either autonomous navigation, or common day-to-day datasets and to the best of the author's knowledge, no DA work has been conducted on  industrial datasets. Additionally, many of these datasets are barely suitable to reflect the domain gap between the real world and the simulations. This thesis attempts to reduce the knowledge gap by utilizing a relatively industrial dataset. However, the target dataset proposed in the \nameref{datasets} section was collected in a typical household setup, where the environment has a stable lighting and background. Therefore, for future work, it is advised to experiment with images from a real industrial plant. \todo{Modify if we manage to obtain real equipment pics} 

\subsubsection{Other detectors}
As it could be noted from the \nameref{DAobj} section, Faster-RCNN is a de-facto object detector framework used in such problems. Oza et al. \cite{Oza2021} suggested to experiment with single-stage object detectors such as YOLO\cite{Redmon2015a}, SSD\cite{Liu2015}, FCOS\cite{Tian2019} and DETR\cite{Carion2020}. In this thesis, two-stage Faster-RCNN was selected due to its simple plug-and-play compatibility with detectron2\cite{wu2019detectron2} and with the existing DA methods. Additionally, the robustness, that single-stage detectors offer, is not as critical in this thesis. However, there is a potential of extending the application to identify the objects in real time. As it was discussed in the \nameref{obj_detection_section} section, single-stage detectors offer a significant improvement over Faster-RCNN. Upon analyzing the feasibility of implementing other detectors in detectron2 \cite{wu2019detectron2} framework, it was identified that FCOS network has already been adapted for detectron2. Hence, it is believed that among the detectors listed, the conversion into the setup proposed in Figure \ref{mymodel} will be the most straightforward with a FCOS-based detector. 

\subsubsection{Imbalanced classes}
Although the classes were distributed fairly well in the source dataset, huge class imbalance was observed in the target T-LESS dataset, as it can be noted from Figure \ref{tless_distribution_real}. The models 1 and 4 overrepresented and are the largest in the target dataset by a tremendous margin. This could be a potential reason for the performance drop as the disproportionately highly represented classes shifted the alignment of the domains in their favor. Oza et al. proposed re-weighting the classes based on their frequency. However, as the target domain does not contain any labels, it becomes a challenging task to track the frequency \cite{Oza2021}. Luckily, the proposed in the \nameref{mainExperiments} section cross-domain adaptation method leverages pseudo-labels. Therefore, re-weighting the classes can be carried out in stages once pseudo-labeling accuracy improves. 

\subsubsection{Multi-source adaptation}
Another practical problem formulated by Oza et al. is that often the images can be collected from multiple domains. In the scope of the thesis, this could imply collecting datasets from multiple plants with varying environmental conditions, such as rainy weather, snow, daylight and nighttime. These conditions will dramatically impact the detection results. Therefore, more experiments should be carried out before releasing the application into use. One suggested approach to this problem could be carried out by using even more sophisticated augmentations to simulate such environments \cite{imgaug}. 


\subsubsection{Continual learning}
Although the \nameref{cont_learning_section} section addressed the catastrophic forgetting problem in a scalable setup, the experiments were only conducted with well-established and possibly outdated methods. Moreover, according to Parisi et al. \cite{Parisi2018}, current state-of-the-art contunual learning models are still far from being able to learn new tasks as efficiently as biological systems and to this day it remains a challenging problem. Additionally, the method proposed raise an additional constraint. Although the model is able to adapt to the dynamically expanding dataset, the model will scale linearly as new classes are added. Within this thesis, the model was tested on up to 30 classes and the resulted model files consumed nearly 2GB of storage and a few linear layers appended will not double the already substantial file size. Nevertheless, for the sake of optimization and as new continual learning solutions emerge, it is important to evaluate them in the given setup and explore new possible solutions. 

\clearpage


\section{Conclusion} 
In this thesis, a novel cross-domain object detection architecture with continual learning was proposed. The architecture combines the state-of-the-art adaptive  teacher ensembled model and the \todo{write some stuff} 

The codebase for this project will shortly be published to \url{https://github.com/darkhan-s/master-thesis-equipment-detection}. However, the access will be restricted and will be available to certain parties upon request.  

\clearpage




% changed from plain to ieeetr to sort by the order of appearance 
\bibliographystyle{ieeetr}
\bibliography{references_library}

%% Appendices
%% If you don't have appendices, remove \clearpage and \thesisappendix below.
\clearpage

\thesisappendix

\section{Appendices}
TODO
\end{document}
