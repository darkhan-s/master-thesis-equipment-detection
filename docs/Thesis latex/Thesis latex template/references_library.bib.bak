@Article{Oza2021,
  author        = {Poojan Oza and Vishwanath A. Sindagi and Vibashan VS and Vishal M. Patel},
  title         = {Unsupervised Domain Adaptation of Object Detectors: A Survey},
  year          = {2021},
  month         = may,
  abstract      = {Recent advances in deep learning have led to the development of accurate and efficient models for various computer vision applications such as classification, segmentation, and detection. However, learning highly accurate models relies on the availability of large-scale annotated datasets. Due to this, model performance drops drastically when evaluated on label-scarce datasets having visually distinct images, termed as domain adaptation problem. There is a plethora of works to adapt classification and segmentation models to label-scarce target datasets through unsupervised domain adaptation. Considering that detection is a fundamental task in computer vision, many recent works have focused on developing novel domain adaptive detection techniques. Here, we describe in detail the domain adaptation problem for detection and present an extensive survey of the various methods. Furthermore, we highlight strategies proposed and the associated shortcomings. Subsequently, we identify multiple aspects of the problem that are most promising for future research. We believe that this survey shall be valuable to the pattern recognition experts working in the fields of computer vision, biometrics, medical imaging, and autonomous navigation by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research.},
  archiveprefix = {arXiv},
  comment       = {Summary of all the up to date methods in DA},
  eprint        = {2105.13502},
  file          = {:Oza2021 - Unsupervised Domain Adaptation of Object Detectors_ a Survey.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  readstatus    = {read},
  relevance     = {relevant},
}

@Article{Sen2022,
  author        = {Jaydip Sen and Sidra Mehtab and Rajdeep Sen and Abhishek Dutta and Pooja Kherwa and Saheel Ahmed and Pranay Berry and Sahil Khurana and Sonali Singh and David W. W Cadotte and David W. Anderson and Kalum J. Ost and Racheal S. Akinbo and Oladunni A. Daramola and Bongs Lainjo},
  title         = {Machine Learning: Algorithms, Models, and Applications},
  year          = {2022},
  month         = jan,
  abstract      = {Recent times are witnessing rapid development in machine learning algorithm systems, especially in reinforcement learning, natural language processing, computer and robot vision, image processing, speech, and emotional processing and understanding. In tune with the increasing importance and relevance of machine learning models, algorithms, and their applications, and with the emergence of more innovative uses cases of deep learning and artificial intelligence, the current volume presents a few innovative research works and their applications in real world, such as stock trading, medical and healthcare systems, and software automation. The chapters in the book illustrate how machine learning and deep learning algorithms and models are designed, optimized, and deployed. The volume will be useful for advanced graduate and doctoral students, researchers, faculty members of universities, practicing data scientists and data engineers, professionals, and consultants working on the broad areas of machine learning, deep learning, and artificial intelligence.},
  archiveprefix = {arXiv},
  doi           = {10.5772/intechopen.94615},
  eprint        = {2201.01943},
  file          = {:http\://arxiv.org/pdf/2201.01943v1:PDF},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Hwang2018,
  author        = {Tim Hwang},
  title         = {Computational Power and the Social Impact of Artificial Intelligence},
  year          = {2018},
  month         = mar,
  abstract      = {Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.},
  archiveprefix = {arXiv},
  eprint        = {1803.08971},
  file          = {:Hwang2018 - Computational Power and the Social Impact of Artificial Intelligence.pdf:PDF},
  keywords      = {cs.AI, cs.CY},
  primaryclass  = {cs.AI},
}

@Article{Janiesch2021,
  author        = {Christian Janiesch and Patrick Zschech and Kai Heinrich},
  title         = {Machine learning and deep learning},
  year          = {2021},
  month         = apr,
  abstract      = {Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.},
  archiveprefix = {arXiv},
  doi           = {10.1007/s12525-021-00475-2},
  eprint        = {2104.05314},
  file          = {:Janiesch2021 - Machine Learning and Deep Learning.pdf:PDF},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{ima,
  author        = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  title         = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  year          = {2015},
  month         = jun,
  abstract      = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  eprint        = {1506.01497},
  file          = {:Ren2015 - Faster R CNN_ Towards Real Time Object Detection with Region Proposal Networks.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Liu2021,
  author        = {Yen-Cheng Liu and Chih-Yao Ma and Zijian He and Chia-Wen Kuo and Kan Chen and Peizhao Zhang and Bichen Wu and Zsolt Kira and Peter Vajda},
  title         = {Unbiased Teacher for Semi-Supervised Object Detection},
  year          = {2021},
  month         = feb,
  abstract      = {Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.},
  archiveprefix = {arXiv},
  eprint        = {2102.09480},
  file          = {:Liu2021 - Unbiased Teacher for Semi Supervised Object Detection.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{hodan2017tless,
  author  = {Hoda{\v{n}}, Tom{\'a}{\v{s}} and Haluza, Pavel and Obdr{\v{z}}{\'a}lek, {\v{S}}t{\v{e}}p{\'a}n and Matas, Ji{\v{r}}{\'\i} and Lourakis, Manolis and Zabulis, Xenophon},
  journal = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title   = {{T-LESS}: An {RGB-D} Dataset for {6D} Pose Estimation of Texture-less Objects},
  year    = {2017},
}

@Article{Lin2014,
  author        = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
  title         = {Microsoft COCO: Common Objects in Context},
  year          = {2014},
  month         = may,
  note          = {\url{https://cocodataset.org/}, last accessed on 2022-06-30},
  abstract      = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  eprint        = {1405.0312},
  file          = {:Lin2014 - Microsoft COCO_ Common Objects in Context.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  urldate       = {2022-06-30},
}

@Article{Everingham10,
  author  = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  journal = {International Journal of Computer Vision},
  title   = {The Pascal Visual Object Classes (VOC) Challenge},
  year    = {2010},
  month   = jun,
  number  = {2},
  pages   = {303--338},
  volume  = {88},
}

@Article{Li2021,
  author        = {Yu-Jhe Li and Xiaoliang Dai and Chih-Yao Ma and Yen-Cheng Liu and Kan Chen and Bichen Wu and Zijian He and Kris Kitani and Peter Vajda},
  title         = {Cross-Domain Adaptive Teacher for Object Detection},
  year          = {2021},
  month         = nov,
  abstract      = {We address the task of domain adaptation in object detection, where there is a domain gap between a domain with annotations (source) and a domain of interest without annotations (target). As an effective semi-supervised learning method, the teacher-student framework (a student model is supervised by the pseudo labels from a teacher model) has also yielded a large accuracy gain in cross-domain object detection. However, it suffers from the domain shift and generates many low-quality pseudo labels (\textit{e.g.,} false positives), which leads to sub-optimal performance. To mitigate this problem, we propose a teacher-student framework named Adaptive Teacher (AT) which leverages domain adversarial learning and weak-strong data augmentation to address the domain gap. Specifically, we employ feature-level adversarial training in the student model, allowing features derived from the source and target domains to share similar distributions. This process ensures the student model produces domain-invariant features. Furthermore, we apply weak-strong augmentation and mutual learning between the teacher model (taking data from the target domain) and the student model (taking data from both domains). This enables the teacher model to learn the knowledge from the student model without being biased to the source domain. We show that AT demonstrates superiority over existing approaches and even Oracle (fully-supervised) models by a large margin. For example, we achieve 50.9% (49.3%) mAP on Foggy Cityscape (Clipart1K), which is 9.2% (5.2%) and 8.2% (11.0%) higher than previous state-of-the-art and Oracle, respectively.},
  archiveprefix = {arXiv},
  eprint        = {2111.13216},
  file          = {:Li2021 - Cross Domain Adaptive Teacher for Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Misc{wu2019detectron2,
  author       = {Yuxin Wu and Alexander Kirillov and Francisco Massa and Wan-Yen Lo and Ross Girshick},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  title        = {Detectron2},
  year         = {2019},
}

@Article{Liu2015,
  author        = {Wei Liu and Dragomir Anguelov and Dumitru Erhan and Christian Szegedy and Scott Reed and Cheng-Yang Fu and Alexander C. Berg},
  title         = {SSD: Single Shot MultiBox Detector},
  year          = {2015},
  month         = dec,
  abstract      = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  doi           = {10.1007/978-3-319-46448-0_2},
  eprint        = {1512.02325},
  file          = {:Liu2015 - SSD_ Single Shot MultiBox Detector.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{He2015,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Deep Residual Learning for Image Recognition},
  year          = {2015},
  month         = dec,
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  eprint        = {1512.03385},
  file          = {:He2015 - Deep Residual Learning for Image Recognition.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Lin2016,
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  title         = {Feature Pyramid Networks for Object Detection},
  year          = {2016},
  month         = dec,
  abstract      = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archiveprefix = {arXiv},
  eprint        = {1612.03144},
  file          = {:Lin2016 - Feature Pyramid Networks for Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Simonyan2014,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year          = {2014},
  month         = sep,
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  eprint        = {1409.1556},
  file          = {:Simonyan2014 - Very Deep Convolutional Networks for Large Scale Image Recognition.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Russakovsky2014,
  author        = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  title         = {ImageNet Large Scale Visual Recognition Challenge},
  year          = {2014},
  month         = sep,
  abstract      = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  eprint        = {1409.0575},
  file          = {:Russakovsky2014 - ImageNet Large Scale Visual Recognition Challenge.pdf:PDF},
  keywords      = {cs.CV, I.4.8; I.5.2},
  primaryclass  = {cs.CV},
}

@Article{Zhu2017,
  author        = {Jun-Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
  title         = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  year          = {2017},
  month         = mar,
  abstract      = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archiveprefix = {arXiv},
  eprint        = {1703.10593},
  file          = {:Zhu2017 - Unpaired Image to Image Translation Using Cycle Consistent Adversarial Networks.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Ganin2015,
  author        = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and François Laviolette and Mario Marchand and Victor Lempitsky},
  journal       = {Journal of Machine Learning Research 2016, vol. 17, p. 1-35},
  title         = {Domain-Adversarial Training of Neural Networks},
  year          = {2015},
  month         = may,
  abstract      = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
  archiveprefix = {arXiv},
  eprint        = {1505.07818},
  file          = {:Ganin2015 - Domain Adversarial Training of Neural Networks.pdf:PDF},
  keywords      = {stat.ML, cs.LG, cs.NE},
  primaryclass  = {stat.ML},
}

@InProceedings{9689485,
  author    = {Saidnassim, Nurbek and Abdikenov, Beibit and Kelesbekov, Rauan and Akhtar, Muhammad Tahir and Jamwal, Prashant},
  booktitle = {2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  title     = {Self-supervised Visual Transformers for Breast Cancer Diagnosis},
  year      = {2021},
  pages     = {423-427},
}

@Online{metso_outotec_2022_1,
  journal = {Metso Outotec},
  note    = {\url{https://www.mogroup.com/portfolio/frothsense/}, last accessed on 2022-06-20},
  title   = {{Metso Outotec: FrothSense}},
  urldate = {2022-06-20},
  year    = {2022},
}

@Article{Etiemble2022,
  author        = {Daniel Etiemble},
  title         = {Technologies and Computing Paradigms: Beyond Moore's law?},
  year          = {2022},
  month         = jun,
  abstract      = {As it is pretty sure that Moore's law will end some day, questioning about the post-Moore era is more than interesting. Similarly, looking for new computing paradigms that could provide solutions is important. Revisiting the history of digital electronics since the 60's provide significant insights on the conditions for the success of a new emerging technology to replace the currently dominant one. Specifically, the past shows when constraints and {\guillemotleft} walls {\guillemotright} have contribute to evolution through improved technical techniques and when they have provoked changes of technologies (evolution versus breakthrough). The main criteria for a new technology or a new computing paradigm is a significant performance improvement (at least one order of magnitude). Cost, space requirement, power and scalability are the other important parameters.},
  archiveprefix = {arXiv},
  eprint        = {2206.03201},
  file          = {:Etiemble2022 - Technologies and Computing Paradigms_ beyond Moore's Law_.pdf:PDF},
  keywords      = {cs.ET, B.0; B.7; C.0; C.5},
  primaryclass  = {cs.ET},
}

@Book{Mehlig_2021,
  author    = {Bernhard Mehlig},
  publisher = {Cambridge University Press},
  title     = {Machine Learning with Neural Networks},
  year      = {2021},
  month     = {oct},
  doi       = {10.1017/9781108860604},
  url       = {https://doi.org/10.1017%2F9781108860604},
}

@Article{Awalgaonkar2020,
  author        = {Nimish M. Awalgaonkar and Haining Zheng and Christopher S. Gurciullo},
  title         = {DEEVA: A Deep Learning and IoT Based Computer Vision System to Address Safety and Security of Production Sites in Energy Industry},
  year          = {2020},
  month         = mar,
  abstract      = {When it comes to addressing the safety/security related needs at different production/construction sites, accurate detection of the presence of workers, vehicles, equipment important and formed an integral part of computer vision-based surveillance systems (CVSS). Traditional CVSS systems focus on the use of different computer vision and pattern recognition algorithms overly reliant on manual extraction of features and small datasets, limiting their usage because of low accuracy, need for expert knowledge and high computational costs. The main objective of this paper is to provide decision makers at sites with a practical yet comprehensive deep learning and IoT based solution to tackle various computer vision related problems such as scene classification, object detection in scenes, semantic segmentation, scene captioning etc. Our overarching goal is to address the central question of What is happening at this site and where is it happening in an automated fashion minimizing the need for human resources dedicated to surveillance. We developed Deep ExxonMobil Eye for Video Analysis (DEEVA) package to handle scene classification, object detection, semantic segmentation and captioning of scenes in a hierarchical approach. The results reveal that transfer learning with the RetinaNet object detector is able to detect the presence of workers, different types of vehicles/construction equipment, safety related objects at a high level of accuracy (above 90%). With the help of deep learning to automatically extract features and IoT technology to automatic capture, transfer and process vast amount of realtime images, this framework is an important step towards the development of intelligent surveillance systems aimed at addressing myriads of open ended problems in the realm of security/safety monitoring, productivity assessments and future decision making.},
  archiveprefix = {arXiv},
  eprint        = {2003.01196},
  file          = {:Awalgaonkar2020 - DEEVA_ a Deep Learning and IoT Based Computer Vision System to Address Safety and Security of Production Sites in Energy Industry.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Janai2017,
  author        = {Joel Janai and Fatma Güney and Aseem Behl and Andreas Geiger},
  title         = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art},
  year          = {2017},
  month         = apr,
  abstract      = {Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This book attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.},
  archiveprefix = {arXiv},
  eprint        = {1704.05519},
  file          = {:Janai2017 - Computer Vision for Autonomous Vehicles_ Problems, Datasets and State of the Art.pdf:PDF},
  keywords      = {cs.CV, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Banf2022,
  author        = {Michael Banf and Gregor Steinhagen},
  title         = {Who supervises the supervisor? Model monitoring in production using deep feature embeddings with applications to workpiece inspection},
  year          = {2022},
  month         = jan,
  abstract      = {The automation of condition monitoring and workpiece inspection plays an essential role in maintaining high quality as well as high throughput of the manufacturing process. To this end, the recent rise of developments in machine learning has lead to vast improvements in the area of autonomous process supervision. However, the more complex and powerful these models become, the less transparent and explainable they generally are as well. One of the main challenges is the monitoring of live deployments of these machine learning systems and raising alerts when encountering events that might impact model performance. In particular, supervised classifiers are typically build under the assumption of stationarity in the underlying data distribution. For example, a visual inspection system trained on a set of material surface defects generally does not adapt or even recognize gradual changes in the data distribution - an issue known as "data drift" - such as the emergence of new types of surface defects. This, in turn, may lead to detrimental mispredictions, e.g. samples from new defect classes being classified as non-defective. To this end, it is desirable to provide real-time tracking of a classifier's performance to inform about the putative onset of additional error classes and the necessity for manual intervention with respect to classifier re-training. Here, we propose an unsupervised framework that acts on top of a supervised classification system, thereby harnessing its internal deep feature representations as a proxy to track changes in the data distribution during deployment and, hence, to anticipate classifier performance degradation.},
  archiveprefix = {arXiv},
  eprint        = {2201.06599},
  file          = {:Banf2022 - Who Supervises the Supervisor_ Model Monitoring in Production Using Deep Feature Embeddings with Applications to Workpiece Inspection.pdf:PDF},
  keywords      = {cs.LG, cs.CV},
  primaryclass  = {cs.LG},
}

@Online{metso_outotec_2022,
  journal = {Metso Outotec},
  note    = {\url{https://www.mogroup.com/portfolio/rocksense/}, last accessed on 2022-06-20},
  title   = {{Metso Outotec: Rocksense}},
  urldate = {2022.06.20},
  year    = {2022},
}

@Online{metso_outotec_2022_2,
  journal = {Metso Outotec},
  note    = {\url{https://www.mogroup.com/portfolio/mouldsense/}, last accessed on 2022-06-20},
  title   = {{Metso Outotec: MouldSense}},
  urldate = {2022-06-20},
  year    = {2022},
}

@Article{article1,
  author  = {Meng, Zhenzhu and Hu, Yating and Ancey, Christophe},
  journal = {Water},
  title   = {Using a Data Driven Approach to Predict Waves Generated by Gravity Driven Mass Flows},
  year    = {2020},
  month   = {02},
  volume  = {12},
  doi     = {10.3390/w12020600},
}

@Article{Rumelhart:1986we,
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  journal   = {Nature},
  title     = {{Learning Representations by Back-propagating Errors}},
  year      = {1986},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  added-at  = {2019-05-21T10:10:49.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi       = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  keywords  = {imported},
  timestamp = {2019-05-21T10:10:49.000+0200},
  url       = {http://www.nature.com/articles/323533a0},
}

@Article{Dubey2021,
  author        = {Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
  title         = {A Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning},
  year          = {2021},
  month         = sep,
  abstract      = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: \url{https://github.com/shivram1987/ActivationFunctions}.},
  archiveprefix = {arXiv},
  eprint        = {2109.14545},
  file          = {:Dubey2021 - A Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning.pdf:PDF},
  keywords      = {cs.LG, cs.NE},
  primaryclass  = {cs.LG},
}

@Article{Albarghouthi2021,
  author        = {Aws Albarghouthi},
  title         = {Introduction to Neural Network Verification},
  year          = {2021},
  month         = sep,
  abstract      = {Deep learning has transformed the way we think of software and what it can do. But deep neural networks are fragile and their behaviors are often surprising. In many settings, we need to provide formal guarantees on the safety, security, correctness, or robustness of neural networks. This book covers foundational ideas from formal verification and their adaptation to reasoning about neural networks and deep learning.},
  archiveprefix = {arXiv},
  eprint        = {2109.10317},
  file          = {:Albarghouthi2021 - Introduction to Neural Network Verification.pdf:PDF},
  keywords      = {cs.LG, cs.AI, cs.PL},
  primaryclass  = {cs.LG},
}

@Article{Alber2018,
  author        = {Maximilian Alber and Irwan Bello and Barret Zoph and Pieter-Jan Kindermans and Prajit Ramachandran and Quoc Le},
  title         = {Backprop Evolution},
  year          = {2018},
  month         = aug,
  abstract      = {The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. We use a domain specific lan- guage to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization per- formance after a few epochs of training. We find several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.},
  archiveprefix = {arXiv},
  eprint        = {1808.02822},
  file          = {:Alber2018 - Backprop Evolution.pdf:PDF},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
}

@Article{Mahony2019,
  author        = {Niall O' Mahony and Sean Campbell and Anderson Carvalho and Suman Harapanahalli and Gustavo Velasco-Hernandez and Lenka Krpalkova and Daniel Riordan and Joseph Walsh},
  journal       = {in Advances in Computer Vision Proceedings of the 2019 Computer Vision Conference (CVC). Springer Nature Switzerland AG, pp. 128-144},
  title         = {Deep Learning vs. Traditional Computer Vision},
  year          = {2019},
  month         = oct,
  abstract      = {Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised},
  archiveprefix = {arXiv},
  doi           = {10.1007/978-3-030-17795-9},
  eprint        = {1910.13796},
  file          = {:Mahony2019 - Deep Learning Vs. Traditional Computer Vision.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{OShea2015,
  author        = {Keiron O'Shea and Ryan Nash},
  title         = {An Introduction to Convolutional Neural Networks},
  year          = {2015},
  month         = nov,
  abstract      = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  archiveprefix = {arXiv},
  eprint        = {1511.08458},
  file          = {:OShea2015 - An Introduction to Convolutional Neural Networks.pdf:PDF},
  keywords      = {cs.NE, cs.CV, cs.LG},
  primaryclass  = {cs.NE},
}

@Online{paperswithcode:2022,
  journal = {{Papers With Code}},
  month   = jun,
  note    = {\url{https://paperswithcode.com/area/computer-vision}, last accessed on 2022-06-20},
  title   = {{Computer Vision}},
  urldate = {2022-06-20},
  year    = {2022},
}

@Article{lecun-mnisthandwrittendigit-2010,
  author       = {LeCun, Yann and Cortes, Corinna},
  title        = {{MNIST} handwritten digit database},
  year         = {2010},
  added-at     = {2010-06-28T21:16:30.000+0200},
  biburl       = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups       = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash    = {21b9d0558bd66279df9452562df6e6f3},
  intrahash    = {935bad99fa1f65e03c25b315aa3c1032},
  keywords     = {MSc _checked character_recognition mnist network neural},
  lastchecked  = {2016-01-14 14:24:11},
  timestamp    = {2016-07-12T19:25:30.000+0200},
  url          = {http://yann.lecun.com/exdb/mnist/},
  username     = {mhwombat},
}

@Article{Liu2016,
  author        = {Mengchen Liu and Jiaxin Shi and Zhen Li and Chongxuan Li and Jun Zhu and Shixia Liu},
  title         = {Towards Better Analysis of Deep Convolutional Neural Networks},
  year          = {2016},
  month         = apr,
  abstract      = {Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.},
  archiveprefix = {arXiv},
  eprint        = {1604.07043},
  file          = {:Liu2016 - Towards Better Analysis of Deep Convolutional Neural Networks.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{alom01,
  author  = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
  journal = {Electronics},
  title   = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
  year    = {2019},
  month   = {03},
  pages   = {292},
  volume  = {8},
  doi     = {10.3390/electronics8030292},
}

@InProceedings{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@InProceedings{lecun-gradientbased-learning-applied-1998,
  author      = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  booktitle   = {Proceedings of the IEEE},
  title       = {Gradient-Based Learning Applied to Document Recognition},
  year        = {1998},
  number      = {11},
  pages       = {2278--2324},
  volume      = {86},
  added-at    = {2010-06-28T21:14:36.000+0200},
  biburl      = {https://www.bibsonomy.org/bibtex/29aa18bc67d862bdb83b6081e5506f050/mhwombat},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  file        = {:neural_nets/lecun-98.pdf:PDF;:lecun-98.pdf:PDF},
  groups      = {public},
  interhash   = {7a82cccacd23cf06b25ff5325a6c86c7},
  intrahash   = {9aa18bc67d862bdb83b6081e5506f050},
  keywords    = {MSc character_recognition checked mnist network neural},
  timestamp   = {2016-07-12T19:25:30.000+0200},
  url         = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  username    = {mhwombat},
}

@Article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  journal = {Journal of Machine Learning Research},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year    = {2014},
  number  = {56},
  pages   = {1929--1958},
  volume  = {15},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@Online{resnet50,
  author  = {Aditi Rastogi},
  journal = {Medium},
  note    = {\url{https://blog.devgenius.io/resnet50-6b42934db431/}, last accessed on 2022-06-20},
  title   = {Medium: ResNet50},
  urldate = {2022-06-20},
  year    = {2022},
}

@Online{obj_det_history,
  author  = {Chinmoy Borah},
  journal = {Medium},
  note    = {\url{https://medium.com/analytics-vidhya/evolution-of-object-detection-582259d2aa9b/}, last accessed on 2022-06-21},
  title   = {Medium: Evolution of Object Detection},
  urldate = {2022-06-21},
  year    = {2022},
}

@Article{Girshick2015,
  author        = {Ross Girshick},
  title         = {Fast R-CNN},
  year          = {2015},
  month         = apr,
  abstract      = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  eprint        = {1504.08083},
  file          = {:- Fast R CNN.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Girshick2013,
  author        = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  title         = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year          = {2013},
  month         = nov,
  abstract      = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  archiveprefix = {arXiv},
  eprint        = {1311.2524},
  file          = {:Girshick2013 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Redmon2018a,
  author        = {Joseph Redmon and Ali Farhadi},
  title         = {YOLOv3: An Incremental Improvement},
  year          = {2018},
  month         = apr,
  abstract      = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  archiveprefix = {arXiv},
  eprint        = {1804.02767},
  file          = {:http\://arxiv.org/pdf/1804.02767v1:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Redmon2015a,
  author        = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title         = {You Only Look Once: Unified, Real-Time Object Detection},
  year          = {2015},
  month         = jun,
  abstract      = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  eprint        = {1506.02640},
  file          = {:- You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Zaidi2021,
  author        = {Syed Sahil Abbas Zaidi and Mohammad Samar Ansari and Asra Aslam and Nadia Kanwal and Mamoona Asghar and Brian Lee},
  title         = {A Survey of Modern Deep Learning based Object Detection Models},
  year          = {2021},
  month         = apr,
  abstract      = {Object Detection is the task of classification and localization of objects in an image or video. It has gained prominence in recent years due to its widespread applications. This article surveys recent developments in deep learning based object detectors. Concise overview of benchmark datasets and evaluation metrics used in detection is also provided along with some of the prominent backbone architectures used in recognition tasks. It also covers contemporary lightweight classification models used on edge devices. Lastly, we compare the performances of these architectures on multiple metrics.},
  archiveprefix = {arXiv},
  eprint        = {2104.11892},
  file          = {:Zaidi2021 - A Survey of Modern Deep Learning Based Object Detection Models.pdf:PDF},
  keywords      = {cs.CV, cs.LG, eess.IV},
  primaryclass  = {cs.CV},
}

@Article{app8091488,
  author         = {Pacha, Alexander and Hajič, Jan and Calvo-Zaragoza, Jorge},
  journal        = {Applied Sciences},
  title          = {A Baseline for General Music Object Detection with Deep Learning},
  year           = {2018},
  issn           = {2076-3417},
  number         = {9},
  volume         = {8},
  abstract       = {Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.},
  article-number = {1488},
  doi            = {10.3390/app8091488},
  url            = {https://www.mdpi.com/2076-3417/8/9/1488},
}

@Article{Lin2017,
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  title         = {Focal Loss for Dense Object Detection},
  year          = {2017},
  month         = aug,
  abstract      = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  eprint        = {1708.02002},
  file          = {:Lin2017 - Focal Loss for Dense Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Uijlings13,
  author    = {J.R.R. Uijlings and K.E.A. van de Sande and T. Gevers and A.W.M. Smeulders},
  journal   = {International Journal of Computer Vision},
  title     = {Selective Search for Object Recognition},
  year      = {2013},
  doi       = {10.1007/s11263-013-0620-5},
  owner     = {jrruijli},
  timestamp = {2013.02.06},
  url       = {http://www.huppelen.nl/publications/selectiveSearchDraft.pdf},
}

@Article{Hosang2017,
  author        = {Jan Hosang and Rodrigo Benenson and Bernt Schiele},
  title         = {Learning non-maximum suppression},
  year          = {2017},
  month         = may,
  abstract      = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and -- being based on greedy clustering with a fixed distance threshold -- forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
  archiveprefix = {arXiv},
  eprint        = {1705.02950},
  file          = {:Hosang2017 - Learning Non Maximum Suppression.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Online{paperswithcode_1:2022,
  journal = {{Papers With Code}},
  month   = jun,
  note    = {\url{https://paperswithcode.com/method/faster-r-cnn}, last accessed on 2022-06-22},
  title   = {{Faster-RCNN}},
  urldate = {2022-06-22},
  year    = {2022},
}

@Article{Redmon2016,
  author        = {Joseph Redmon and Ali Farhadi},
  title         = {YOLO9000: Better, Faster, Stronger},
  year          = {2016},
  month         = dec,
  abstract      = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix = {arXiv},
  eprint        = {1612.08242},
  file          = {:Redmon2016 - YOLO9000_ Better, Faster, Stronger.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Liu2021a,
  author        = {Ziquan Liu and Yi Xu and Yuanhong Xu and Qi Qian and Hao Li and Xiangyang Ji and Antoni Chan and Rong Jin},
  title         = {Improved Fine-Tuning by Better Leveraging Pre-Training Data},
  year          = {2021},
  month         = nov,
  abstract      = {As a dominant paradigm, fine-tuning a pre-trained model on the target data is widely used in many deep learning applications, especially for small data sets. However, recent studies have empirically shown that training from scratch has the final performance that is no worse than this pre-training strategy once the number of training samples is increased in some vision tasks. In this work, we revisit this phenomenon from the perspective of generalization analysis by using excess risk bound which is popular in learning theory. The result reveals that the excess risk bound may have a weak dependency on the pre-trained model. The observation inspires us to leverage pre-training data for fine-tuning, since this data is also available for fine-tuning. The generalization result of using pre-training data shows that the excess risk bound on a target task can be improved when the appropriate pre-training data is included in fine-tuning. With the theoretical motivation, we propose a novel selection strategy to select a subset from pre-training data to help improve the generalization on the target task. Extensive experimental results for image classification tasks on 8 benchmark data sets verify the effectiveness of the proposed data selection based fine-tuning pipeline.},
  archiveprefix = {arXiv},
  eprint        = {2111.12292},
  file          = {:Liu2021a - Improved Fine Tuning by Better Leveraging Pre Training Data.pdf:PDF},
  keywords      = {cs.CV, cs.LG, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Zhang2021,
  author        = {Youshan Zhang},
  title         = {A Survey of Unsupervised Domain Adaptation for Visual Recognition},
  year          = {2021},
  month         = dec,
  abstract      = {While huge volumes of unlabeled data are generated and made available in many domains, the demand for automated understanding of visual data is higher than ever before. Most existing machine learning models typically rely on massive amounts of labeled training data to achieve high performance. Unfortunately, such a requirement cannot be met in real-world applications. The number of labels is limited and manually annotating data is expensive and time-consuming. It is often necessary to transfer knowledge from an existing labeled domain to a new domain. However, model performance degrades because of the differences between domains (domain shift or dataset bias). To overcome the burden of annotation, Domain Adaptation (DA) aims to mitigate the domain shift problem when transferring knowledge from one domain into another similar but different domain. Unsupervised DA (UDA) deals with a labeled source domain and an unlabeled target domain. The principal objective of UDA is to reduce the domain discrepancy between the labeled source data and unlabeled target data and to learn domain-invariant representations across the two domains during training. In this paper, we first define UDA problem. Secondly, we overview the state-of-the-art methods for different categories of UDA from both traditional methods and deep learning based methods. Finally, we collect frequently used benchmark datasets and report results of the state-of-the-art methods of UDA on visual recognition problem.},
  archiveprefix = {arXiv},
  eprint        = {2112.06745},
  file          = {:- A Survey of Unsupervised Domain Adaptation for Visual Recognition.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Wang2018,
  author        = {Mei Wang and Weihong Deng},
  journal       = {Neurocomputing, 2018, 312: 135-153},
  title         = {Deep Visual Domain Adaptation: A Survey},
  year          = {2018},
  month         = feb,
  abstract      = {Deep domain adaption has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaption methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaption, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaption scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaption approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.},
  archiveprefix = {arXiv},
  doi           = {10.1016/j.neucom.2018.05.083},
  eprint        = {1802.03601},
  file          = {:Wang2018 - Deep Visual Domain Adaptation_ a Survey.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Rezaeianaran2021,
  author        = {Farzaneh Rezaeianaran and Rakshith Shetty and Rahaf Aljundi and Daniel Olmeda Reino and Shanshan Zhang and Bernt Schiele},
  title         = {Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection},
  year          = {2021},
  month         = oct,
  abstract      = {In order to robustly deploy object detectors across a wide range of scenarios, they should be adaptable to shifts in the input distribution without the need to constantly annotate new data. This has motivated research in Unsupervised Domain Adaptation (UDA) algorithms for detection. UDA methods learn to adapt from labeled source domains to unlabeled target domains, by inducing alignment between detector features from source and target domains. Yet, there is no consensus on what features to align and how to do the alignment. In our work, we propose a framework that generalizes the different components commonly used by UDA methods laying the ground for an in-depth analysis of the UDA design space. Specifically, we propose a novel UDA algorithm, ViSGA, a direct implementation of our framework, that leverages the best design choices and introduces a simple but effective method to aggregate features at instance-level based on visual similarity before inducing group alignment via adversarial training. We show that both similarity-based grouping and adversarial training allows our model to focus on coarsely aligning feature groups, without being forced to match all instances across loosely aligned domains. Finally, we examine the applicability of ViSGA to the setting where labeled data are gathered from different sources. Experiments show that not only our method outperforms previous single-source approaches on Sim2Real and Adverse Weather, but also generalizes well to the multi-source setting.},
  archiveprefix = {arXiv},
  eprint        = {2110.01428},
  file          = {:Rezaeianaran2021 - Seeking Similarities Over Differences_ Similarity Based Domain Alignment for Adaptive Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Khodabandeh2019,
  author        = {Mehran Khodabandeh and Arash Vahdat and Mani Ranjbar and William G. Macready},
  title         = {A Robust Learning Approach to Domain Adaptive Object Detection},
  year          = {2019},
  month         = apr,
  abstract      = {Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.},
  archiveprefix = {arXiv},
  eprint        = {1904.02361},
  file          = {:Khodabandeh2019 - A Robust Learning Approach to Domain Adaptive Object Detection.pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Hsu2019,
  author        = {Han-Kai Hsu and Chun-Han Yao and Yi-Hsuan Tsai and Wei-Chih Hung and Hung-Yu Tseng and Maneesh Singh and Ming-Hsuan Yang},
  title         = {Progressive Domain Adaptation for Object Detection},
  year          = {2019},
  month         = oct,
  abstract      = {Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution. Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal results. In this paper, we propose to bridge the domain gap with an intermediate domain and progressively solve easier adaptation subtasks. This intermediate domain is constructed by translating the source images to mimic the ones in the target domain. To tackle the domain-shift problem, we adopt adversarial learning to align distributions at the feature level. In addition, a weighted task loss is applied to deal with unbalanced image quality in the intermediate domain. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the performance on the target domain.},
  archiveprefix = {arXiv},
  eprint        = {1910.11319},
  file          = {:Hsu2019 - Progressive Domain Adaptation for Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Cai2019,
  author        = {Qi Cai and Yingwei Pan and Chong-Wah Ngo and Xinmei Tian and Lingyu Duan and Ting Yao},
  title         = {Exploring Object Relation in Mean Teacher for Cross-Domain Detection},
  year          = {2019},
  month         = apr,
  abstract      = {Rendering synthetic data (e.g., 3D CAD-rendered images) to generate annotations for learning deep models in vision tasks has attracted increasing attention in recent years. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. To address this issue, recent progress in cross-domain recognition has featured the Mean Teacher, which directly simulates unsupervised domain adaptation as semi-supervised learning. The domain gap is thus naturally bridged with consistency regularization in a teacher-student scheme. In this work, we advance this Mean Teacher paradigm to be applicable for cross-domain detection. Specifically, we present Mean Teacher with Object Relations (MTOR) that novelly remolds Mean Teacher under the backbone of Faster R-CNN by integrating the object relations into the measure of consistency cost between teacher and student modules. Technically, MTOR firstly learns relational graphs that capture similarities between pairs of regions for teacher and student respectively. The whole architecture is then optimized with three consistency regularizations: 1) region-level consistency to align the region-level predictions between teacher and student, 2) inter-graph consistency for matching the graph structures between teacher and student, and 3) intra-graph consistency to enhance the similarity between regions of same class within the graph of student. Extensive experiments are conducted on the transfers across Cityscapes, Foggy Cityscapes, and SIM10k, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain a new record of single model: 22.8% of mAP on Syn2Real detection dataset.},
  archiveprefix = {arXiv},
  eprint        = {1904.11245},
  file          = {:Cai2019 - Exploring Object Relation in Mean Teacher for Cross Domain Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Kim2019,
  author        = {Taekyung Kim and Minki Jeong and Seunghyeon Kim and Seokeon Choi and Changick Kim},
  title         = {Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection},
  year          = {2019},
  month         = may,
  abstract      = {We introduce a novel unsupervised domain adaptation approach for object detection. We aim to alleviate the imperfect translation problem of pixel-level adaptations, and the source-biased discriminativity problem of feature-level adaptations simultaneously. Our approach is composed of two stages, i.e., Domain Diversification (DD) and Multi-domain-invariant Representation Learning (MRL). At the DD stage, we diversify the distribution of the labeled data by generating various distinctive shifted domains from the source domain. At the MRL stage, we apply adversarial learning with a multi-domain discriminator to encourage feature to be indistinguishable among the domains. DD addresses the source-biased discriminativity, while MRL mitigates the imperfect image translation. We construct a structured domain adaptation framework for our learning paradigm and introduce a practical way of DD for implementation. Our method outperforms the state-of-the-art methods by a large margin of 3%~11% in terms of mean average precision (mAP) on various datasets.},
  archiveprefix = {arXiv},
  eprint        = {1905.05396},
  file          = {:Kim2019 - Diversify and Match_ a Domain Adaptive Representation Learning Paradigm for Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Zhuang2019,
  author        = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
  title         = {A Comprehensive Survey on Transfer Learning},
  year          = {2019},
  month         = nov,
  abstract      = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archiveprefix = {arXiv},
  eprint        = {1911.02685},
  file          = {:Zhuang2019 - A Comprehensive Survey on Transfer Learning.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{SUN201584,
  author   = {Shiliang Sun and Honglei Shi and Yuanbin Wu},
  journal  = {Information Fusion},
  title    = {A survey of multi-source domain adaptation},
  year     = {2015},
  issn     = {1566-2535},
  pages    = {84-92},
  volume   = {24},
  abstract = {In many machine learning algorithms, a major assumption is that the training and the test samples are in the same feature space and have the same distribution. However, for many real applications this assumption does not hold. In this paper, we survey the problem where the training samples and the test samples are from different distributions. This problem can be referred as domain adaptation. The training samples, always with labels, are obtained from what is called source domains, while the test samples, which usually have no labels or only a few labels, are obtained from what is called target domains. The source domains and the target domains are different but related to some extent; the learners can learn some information from the source domains for the learning of the target domains. We focus on the multi-source domain adaptation problem where there is more than one source domain available together with only one target domain. A key issue is how to select good sources and samples for the adaptation. In this survey, we review some theoretical results and well developed algorithms for the multi-source domain adaptation problem. We also discuss some open problems which can be explored in future work.},
  doi      = {https://doi.org/10.1016/j.inffus.2014.12.003},
  keywords = {Machine learning, Multi-source learning, Domain adaptation, Transfer learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253514001316},
}

@Article{Carion2020,
  author        = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  title         = {End-to-End Object Detection with Transformers},
  year          = {2020},
  month         = may,
  abstract      = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archiveprefix = {arXiv},
  eprint        = {2005.12872},
  file          = {:Carion2020 - End to End Object Detection with Transformers.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Tian2019,
  author        = {Zhi Tian and Chunhua Shen and Hao Chen and Tong He},
  title         = {FCOS: Fully Convolutional One-Stage Object Detection},
  year          = {2019},
  month         = apr,
  abstract      = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1},
  archiveprefix = {arXiv},
  eprint        = {1904.01355},
  file          = {:Tian2019 - FCOS_ Fully Convolutional One Stage Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{He2017,
  author        = {Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
  title         = {Mask R-CNN},
  year          = {2017},
  month         = mar,
  abstract      = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arXiv},
  eprint        = {1703.06870},
  file          = {:He2017 - Mask R CNN.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Chen2018,
  author        = {Yuhua Chen and Wen Li and Christos Sakaridis and Dengxin Dai and Luc Van Gool},
  title         = {Domain Adaptive Faster R-CNN for Object Detection in the Wild},
  year          = {2018},
  month         = mar,
  abstract      = {Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.},
  archiveprefix = {arXiv},
  eprint        = {1803.03243},
  file          = {:Chen2018 - Domain Adaptive Faster R CNN for Object Detection in the Wild.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Xu2020,
  author        = {Chang-Dong Xu and Xing-Ran Zhao and Xin Jin and Xiu-Shen Wei},
  title         = {Exploring Categorical Regularization for Domain Adaptive Object Detection},
  year          = {2020},
  month         = mar,
  abstract      = {In this paper, we tackle the domain adaptive object detection problem, where the main challenge lies in significant domain gaps between source and target domains. Previous work seeks to plainly align image-level and instance-level shifts to eventually minimize the domain discrepancy. However, they still overlook to match crucial image regions and important instances across domains, which will strongly affect domain shift mitigation. In this work, we propose a simple but effective categorical regularization framework for alleviating this issue. It can be applied as a plug-and-play component on a series of Domain Adaptive Faster R-CNN methods which are prominent for dealing with domain adaptive detection. Specifically, by integrating an image-level multi-label classifier upon the detection backbone, we can obtain the sparse but crucial image regions corresponding to categorical information, thanks to the weakly localization ability of the classification manner. Meanwhile, at the instance level, we leverage the categorical consistency between image-level predictions (by the classifier) and instance-level predictions (by the detection head) as a regularization factor to automatically hunt for the hard aligned instances of target domains. Extensive experiments of various domain shift scenarios show that our method obtains a significant performance gain over original Domain Adaptive Faster R-CNN detectors. Furthermore, qualitative visualization and analyses can demonstrate the ability of our method for attending on the key regions/instances targeting on domain adaptation. Our code is open-source and available at \url{https://github.com/Megvii-Nanjing/CR-DA-DET}.},
  archiveprefix = {arXiv},
  eprint        = {2003.09152},
  file          = {:Xu2020 - Exploring Categorical Regularization for Domain Adaptive Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Saito2018,
  author        = {Kuniaki Saito and Yoshitaka Ushiku and Tatsuya Harada and Kate Saenko},
  title         = {Strong-Weak Distribution Alignment for Adaptive Object Detection},
  year          = {2018},
  month         = dec,
  abstract      = {We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at \url{https://github.com/VisionLearningGroup/DA_Detection}},
  archiveprefix = {arXiv},
  eprint        = {1812.04798},
  file          = {:Saito2018 - Strong Weak Distribution Alignment for Adaptive Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Misc{Detectron2018,
  author       = {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and Piotr Doll\'{a}r and Kaiming He},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  title        = {Detectron},
  year         = {2018},
}

@InCollection{NEURIPS2019_9015,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Article{Parisi2018,
  author        = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
  title         = {Continual Lifelong Learning with Neural Networks: A Review},
  year          = {2018},
  month         = feb,
  abstract      = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  archiveprefix = {arXiv},
  doi           = {10.1016/j.neunet.2019.01.012.},
  eprint        = {1802.07569},
  file          = {:Parisi2018 - Continual Lifelong Learning with Neural Networks_ a Review.pdf:PDF},
  keywords      = {cs.LG, q-bio.NC, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Rusu2016,
  author        = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell},
  title         = {Progressive Neural Networks},
  year          = {2016},
  month         = jun,
  abstract      = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  archiveprefix = {arXiv},
  eprint        = {1606.04671},
  file          = {:Rusu2016 - Progressive Neural Networks.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Razavian2014,
  author        = {Ali Sharif Razavian and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  title         = {CNN Features off-the-shelf: an Astounding Baseline for Recognition},
  year          = {2014},
  month         = mar,
  abstract      = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  archiveprefix = {arXiv},
  eprint        = {1403.6382},
  file          = {:Razavian2014 - CNN Features off the Shelf_ an Astounding Baseline for Recognition.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Donahue2013,
  author        = {Jeff Donahue and Yangqing Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
  title         = {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  year          = {2013},
  month         = oct,
  abstract      = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
  archiveprefix = {arXiv},
  eprint        = {1310.1531},
  file          = {:Donahue2013 - DeCAF_ a Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Yoon2017,
  author        = {Jaehong Yoon and Eunho Yang and Jeongtae Lee and Sung Ju Hwang},
  title         = {Lifelong Learning with Dynamically Expandable Networks},
  year          = {2017},
  month         = aug,
  abstract      = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained significantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.},
  archiveprefix = {arXiv},
  eprint        = {1708.01547},
  file          = {:Yoon2017 - Lifelong Learning with Dynamically Expandable Networks.pdf:PDF},
  keywords      = {cs.LG, I.2.6, I.2.10},
  primaryclass  = {cs.LG},
}

@Article{Pan2010,
  author  = {Pan, Sinno Jialin and Yang, Qiang},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {A Survey on Transfer Learning},
  year    = {2010},
  number  = {10},
  pages   = {1345-1359},
  volume  = {22},
  doi     = {10.1109/TKDE.2009.191},
}

@Article{Vidit2021,
  author        = {Vidit Vidit and Mathieu Salzmann},
  title         = {Attention-based Domain Adaptation for Single Stage Detectors},
  year          = {2021},
  month         = jun,
  abstract      = {While domain adaptation has been used to improve the performance of object detectors when the training and test data follow different distributions, previous work has mostly focused on two-stage detectors. This is because their use of region proposals makes it possible to perform local adaptation, which has been shown to significantly improve the adaptation effectiveness. Here, by contrast, we target single-stage architectures, which are better suited to resource-constrained detection than two-stage ones but do not provide region proposals. To nonetheless benefit from the strength of local adaptation, we introduce an attention mechanism that lets us identify the important regions on which adaptation should focus. Our method gradually adapts the features from global, image-level to local, instance-level. Our approach is generic and can be integrated into any single-stage detector. We demonstrate this on standard benchmark datasets by applying it to both SSD and YOLOv5. Furthermore, for equivalent single-stage architectures, our method outperforms the state-of-the-art domain adaptation techniques even though they were designed for specific detectors.},
  archiveprefix = {arXiv},
  eprint        = {2106.07283},
  file          = {:Vidit2021 - Attention Based Domain Adaptation for Single Stage Detectors.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Zhang2021a,
  author        = {Shizhao Zhang and Hongya Tuo and Jian Hu and Zhongliang Jing},
  title         = {Domain Adaptive YOLO for One-Stage Cross-Domain Detection},
  year          = {2021},
  month         = jun,
  abstract      = {Domain shift is a major challenge for object detectors to generalize well to real world applications. Emerging techniques of domain adaptation for two-stage detectors help to tackle this problem. However, two-stage detectors are not the first choice for industrial applications due to its long time consumption. In this paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve cross-domain performance for one-stage detectors. Image level features alignment is used to strictly match for local features like texture, and loosely match for global features like illumination. Multi-scale instance level features alignment is presented to reduce instance domain shift effectively , such as variations in object appearance and viewpoint. A consensus regularization to these domain classifiers is employed to help the network generate domain-invariant detections. We evaluate our proposed method on popular datasets like Cityscapes, KITTI, SIM10K and etc.. The results demonstrate significant improvement when tested under different cross-domain scenarios.},
  archiveprefix = {arXiv},
  eprint        = {2106.13939},
  file          = {:Zhang2021a - Domain Adaptive YOLO for One Stage Cross Domain Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Hnewa2021,
  author        = {Mazin Hnewa and Hayder Radha},
  journal       = {IEEE International Conference on Image Processing (ICIP), 2021, pp. 3323-3327,},
  title         = {Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection},
  year          = {2021},
  month         = jun,
  abstract      = {The area of domain adaptation has been instrumental in addressing the domain shift problem encountered by many applications. This problem arises due to the difference between the distributions of source data used for training in comparison with target data used during realistic testing scenarios. In this paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO) framework that employs multiple domain adaptation paths and corresponding domain classifiers at different scales of the recently introduced YOLOv4 object detector to generate domain-invariant features. We train and test our proposed method using popular datasets. Our experiments show significant improvements in object detection performance when training YOLOv4 using the proposed MS-DAYOLO and when tested on target data representing challenging weather conditions for autonomous driving applications.},
  archiveprefix = {arXiv},
  doi           = {10.1109/ICIP42928.2021.9506039},
  eprint        = {2106.01483},
  file          = {:Hnewa2021 - Multiscale Domain Adaptive YOLO for Cross Domain Object Detection.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Zhang2021b,
  author        = {Jingyi Zhang and Jiaxing Huang and Zhipeng Luo and Gongjie Zhang and Shijian Lu},
  title         = {DA-DETR: Domain Adaptive Detection Transformer by Hybrid Attention},
  year          = {2021},
  month         = mar,
  abstract      = {The prevalent approach in domain adaptive object detection adopts a two-stage architecture (Faster R-CNN) that involves a number of hyper-parameters and hand-crafted designs such as anchors, region pooling, non-maximum suppression, etc. Such architecture makes it very complicated while adopting certain existing domain adaptation methods with different ways of feature alignment. In this work, we adopt a one-stage detector and design DA-DETR, a simple yet effective domain adaptive object detection network that performs inter-domain alignment with a single discriminator. DA-DETR introduces a hybrid attention module that explicitly pinpoints the hard-aligned features for simple yet effective alignment across domains. It greatly simplifies traditional domain adaptation pipelines by eliminating sophisticated routines that involve multiple adversarial learning frameworks with different types of features. Despite its simplicity, extensive experiments show that DA-DETR demonstrates superior accuracy as compared with highly-optimized state-of-the-art approaches.},
  archiveprefix = {arXiv},
  eprint        = {2103.17084},
  file          = {:Zhang2021b - DA DETR_ Domain Adaptive Detection Transformer by Hybrid Attention.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Misc{fcos1,
  author    = {Hsu, Cheng-Chun and Tsai, Yi-Hsuan and Lin, Yen-Yu and Yang, Ming-Hsuan},
  title     = {Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2008.08574},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2008.08574},
}

@Online{navisworks,
  journal = {Autodesk Navisworks},
  note    = {\url{https://apidocs.co/apps/navisworks/2018/87317537-2911-4c08-b492-6496c82b3ed0.htm/}, last accessed on 2022-06-29},
  title   = {{Autodesk Navisworks API}},
  urldate = {2022-06-29},
  year    = {2022},
}

@Article{hodan2018bop,
  author  = {Hoda{\v{n}}, Tom{\'a}{\v{s}} and Michel, Frank and Brachmann, Eric and Kehl, Wadim and Glent Buch, Anders and Kraft, Dirk and Drost, Bertram and Vidal, Joel and Ihrke, Stephan and Zabulis, Xenophon and Sahin, Caner and Manhardt, Fabian and Tombari, Federico and Kim, Tae-Kyun and Matas, Ji{\v{r}}{\'i} and Rother, Carsten},
  journal = {European Conference on Computer Vision (ECCV)},
  title   = {{BOP}: Benchmark for {6D} Object Pose Estimation},
  year    = {2018},
}

@Article{hodan2018bop_format,
  author  = {Hoda{\v{n}}, Tom{\'a}{\v{s}} and Michel, Frank and Brachmann, Eric and Kehl, Wadim and Glent Buch, Anders and Kraft, Dirk and Drost, Bertram and Vidal, Joel and Ihrke, Stephan and Zabulis, Xenophon and Sahin, Caner and Manhardt, Fabian and Tombari, Federico and Kim, Tae-Kyun and Matas, Ji{\v{r}}{\'i} and Rother, Carsten},
  journal = {European Conference on Computer Vision (ECCV)},
  title   = {{BOP}: Benchmark for {6D} Object Pose Estimation dataset format},
  year    = {2018},
  note    = {\url{https://github.com/thodan/bop_toolkit/blob/master/docs/bop_datasets_format.md/}, last accessed on 2022-06-29},
  urldate = {2022-06-29},
}

@Article{confusion,
  author  = {Marquand, Andre and Plichta, Michael and Ehlis, Ann-Christine and Schecklmann, Martin and Dresler, Thomas and Jarczok, Tomasz and Eirich, Elisa and Leonhard, Christine and Reif, Andreas and Lesch, Klaus-Peter and Brammer, Michael and Mourão-Miranda, Janaina and Fallgatter, Andreas},
  journal = {Human brain mapping},
  title   = {A Novel Approach to Probabilistic Biomarker-Based Classification Using Functional Near-Infrared Spectroscopy},
  year    = {2013},
  month   = {05},
  volume  = {34},
  doi     = {10.1002/hbm.21497},
}

@Online{mAp_blog,
  author  = {Nick Zeng},
  journal = {NickZeng},
  note    = {\url{https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/}, last accessed on 2022-07-04},
  title   = {{An Introduction to Evaluation Metrics for Object Detection}},
  urldate = {2022-07-04},
  year    = {2022},
}

@Article{Jiang2021,
  author        = {Junguang Jiang and Baixu Chen and Jianmin Wang and Mingsheng Long},
  journal       = {ICLR 2022},
  title         = {Decoupled Adaptation for Cross-Domain Object Detection},
  year          = {2021},
  month         = oct,
  abstract      = {Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we fill the blank of regression domain adaptation in object detection by introducing a bounding box adaptor. Experiments show that D-adapt achieves state-of-the-art results on four cross-domain object detection tasks and yields 17% and 21% relative improvement on benchmark datasets Clipart1k and Comic2k in particular.},
  archiveprefix = {arXiv},
  eprint        = {2110.02578},
  file          = {:Jiang2021 - Decoupled Adaptation for Cross Domain Object Detection.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Chen2020,
  author        = {Chaoqi Chen and Zebiao Zheng and Xinghao Ding and Yue Huang and Qi Dou},
  title         = {Harmonizing Transferability and Discriminability for Adapting Object Detectors},
  year          = {2020},
  month         = mar,
  abstract      = {Recent advances in adaptive object detection have achieved compelling results in virtue of adversarial feature adaptation to mitigate the distributional shifts along the detection pipeline. Whilst adversarial adaptation significantly enhances the transferability of feature representations, the feature discriminability of object detectors remains less investigated. Moreover, transferability and discriminability may come at a contradiction in adversarial adaptation given the complex combinations of objects and the differentiated scene layouts between domains. In this paper, we propose a Hierarchical Transferability Calibration Network (HTCN) that hierarchically (local-region/image/instance) calibrates the transferability of feature representations for harmonizing transferability and discriminability. The proposed model consists of three components: (1) Importance Weighted Adversarial Training with input Interpolation (IWAT-I), which strengthens the global discriminability by re-weighting the interpolated image-level features; (2) Context-aware Instance-Level Alignment (CILA) module, which enhances the local discriminability by capturing the underlying complementary effect between the instance-level feature and the global context information for the instance-level feature alignment; (3) local feature masks that calibrate the local transferability to provide semantic guidance for the following discriminative pattern alignment. Experimental results show that HTCN significantly outperforms the state-of-the-art methods on benchmark datasets.},
  archiveprefix = {arXiv},
  eprint        = {2003.06297},
  file          = {:Chen2020 - Harmonizing Transferability and Discriminability for Adapting Object Detectors.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@InProceedings{Inoue_2018_CVPR,
  author    = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation},
  year      = {2018},
  month     = {June},
}

@Article{Arruda2019,
  author        = {Vinicius F. Arruda and Thiago M. Paixão and Rodrigo F. Berriel and Alberto F. De Souza and Claudine Badue and Nicu Sebe and Thiago Oliveira-Santos},
  title         = {Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: From Day to Night},
  year          = {2019},
  month         = jul,
  abstract      = {Deep learning techniques have enabled the emergence of state-of-the-art models to address object detection tasks. However, these techniques are data-driven, delegating the accuracy to the training dataset which must resemble the images in the target task. The acquisition of a dataset involves annotating images, an arduous and expensive process, generally requiring time and manual effort. Thus, a challenging scenario arises when the target domain of application has no annotated dataset available, making tasks in such situation to lean on a training dataset of a different domain. Sharing this issue, object detection is a vital task for autonomous vehicles where the large amount of driving scenarios yields several domains of application requiring annotated data for the training process. In this work, a method for training a car detection system with annotated data from a source domain (day images) without requiring the image annotations of the target domain (night images) is presented. For that, a model based on Generative Adversarial Networks (GANs) is explored to enable the generation of an artificial dataset with its respective annotations. The artificial dataset (fake dataset) is created translating images from day-time domain to night-time domain. The fake dataset, which comprises annotated images of only the target domain (night images), is then used to train the car detector model. Experimental results showed that the proposed method achieved significant and consistent improvements, including the increasing by more than 10% of the detection performance when compared to the training with only the available annotated data (i.e., day images).},
  archiveprefix = {arXiv},
  doi           = {10.1109/IJCNN.2019.8852008},
  eprint        = {1907.08719},
  file          = {:Arruda2019 - Cross Domain Car Detection Using Unsupervised Image to Image Translation_ from Day to Night.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Loshchilov2016,
  author        = {Ilya Loshchilov and Frank Hutter},
  title         = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  year          = {2016},
  month         = aug,
  abstract      = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arXiv},
  eprint        = {1608.03983},
  file          = {:Loshchilov2016 - SGDR_ Stochastic Gradient Descent with Warm Restarts.pdf:PDF},
  keywords      = {cs.LG, cs.NE, math.OC},
  primaryclass  = {cs.LG},
}

@Article{Shan2018,
  author        = {Yuhu Shan and Wen Feng Lu and Chee Meng Chew},
  title         = {Pixel and Feature Level Based Domain Adaption for Object Detection in Autonomous Driving},
  year          = {2018},
  month         = sep,
  abstract      = {Annotating large scale datasets to train modern convolutional neural networks is prohibitively expensive and time-consuming for many real tasks. One alternative is to train the model on labeled synthetic datasets and apply it in the real scenes. However, this straightforward method often fails to generalize well mainly due to the domain bias between the synthetic and real datasets. Many unsupervised domain adaptation (UDA) methods are introduced to address this problem but most of them only focus on the simple classification task. In this paper, we present a novel UDA model to solve the more complex object detection problem in the context of autonomous driving. Our model integrates both pixel level and feature level based transformtions to fulfill the cross domain detection task and can be further trained end-to-end to pursue better performance. We employ objectives of the generative adversarial network and the cycle consistency loss for image translation in the pixel space. To address the potential semantic inconsistency problem, we propose region proposal based feature adversarial training to preserve the semantics of our target objects as well as further minimize the domain shifts. Extensive experiments are conducted on several different datasets, and the results demonstrate the robustness and superiority of our method.},
  archiveprefix = {arXiv},
  eprint        = {1810.00345},
  file          = {:Shan2018 - Pixel and Feature Level Based Domain Adaption for Object Detection in Autonomous Driving.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Chen2021,
  author  = {Chen, Yuhua and Wang, Haoran and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Gool, Luc},
  journal = {International Journal of Computer Vision},
  title   = {Scale-Aware Domain Adaptive Faster R-CNN},
  year    = {2021},
  month   = {07},
  volume  = {129},
  doi     = {10.1007/s11263-021-01447-x},
}

@Article{Peng2020,
  author   = {Can Peng and Kun Zhao and Brian C. Lovell},
  journal  = {Pattern Recognition Letters},
  title    = {Faster ILOD: Incremental learning for object detectors based on faster RCNN},
  year     = {2020},
  issn     = {0167-8655},
  pages    = {109-115},
  volume   = {140},
  abstract = {The human vision and perception system is inherently incremental where new knowledge is continually learned over time whilst existing knowledge is retained. On the other hand, deep learning networks are ill-equipped for incremental learning. When a well-trained network is adapted to new categories, its performance on the old categories will dramatically degrade. To address this problem, incremental learning methods have been explored which preserve the old knowledge of deep learning models. However, the state-of-the-art incremental object detector employs an external fixed region proposal method that increases overall computation time and reduces accuracy comparing to Region Proposal Network (RPN) based object detectors such as Faster RCNN. The purpose of this paper is to design an efficient end-to-end incremental object detector using knowledge distillation. We first evaluate and analyze the performance of the RPN-based detector with classic distillation on incremental detection tasks. Then, we introduce multi-network adaptive distillation that properly retains knowledge from the old categories when fine-tuning the model for new task. Experiments on the benchmark datasets, PASCAL VOC and COCO, demonstrate that the proposed incremental detector based on Faster RCNN is more accurate as well as being 13 times faster than the baseline detector.},
  doi      = {https://doi.org/10.1016/j.patrec.2020.09.030},
  keywords = {Deep learning, Object detection, Incremental learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167865520303627},
}

@Online{pytorch,
  journal = {Pytorch},
  note    = {\url{https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-download-auto-examples-plot-transforms-py/}, last accessed on 2022-07-11},
  title   = {{Pytorch: Illustration of transforms}},
  urldate = {2022-07-11},
  year    = {2022},
}

@Misc{imgaug,
  author       = {Jung, Alexander B. and Wada, Kentaro and Crall, Jon and Tanaka, Satoshi and Graving, Jake and Reinders, Christoph and Yadav, Sarthak and Banerjee, Joy and Vecsei, Gábor and Kraft, Adam and Rui, Zheng and Borovec, Jirka and Vallentin, Christian and Zhydenko, Semen and Pfeiffer, Kilian and Cook, Ben and Fernández, Ismael and De Rainville, François-Michel and Weng, Chi-Hung and Ayala-Acevedo, Abner and Meudec, Raphael and Laporte, Matias and others},
  howpublished = {\url{https://github.com/aleju/imgaug}},
  note         = {Online; accessed 01-Feb-2020},
  title        = {{imgaug}},
  year         = {2020},
}

@Article{MALBURG2021581,
  author   = {Lukas Malburg and Manfred-Peter Rieder and Ronny Seiger and Patrick Klein and Ralph Bergmann},
  journal  = {Procedia Computer Science},
  title    = {Object Detection for Smart Factory Processes by Machine Learning},
  year     = {2021},
  issn     = {1877-0509},
  note     = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
  pages    = {581-588},
  volume   = {184},
  abstract = {The production industry is in a transformation towards more autonomous and intelligent manufacturing. In addition to more flexible production processes to dynamically respond to changes in the environment, it is also essential that production processes are continuously monitored and completed in time. Video-based methods such as object detection systems are still in their infancy and rarely used as basis for process monitoring. In this paper, we present a framework for video-based monitoring of manufacturing processes with the help of a physical smart factory simulation model. We evaluate three state-of-the-art object detection systems regarding their suitability to detect workpieces and to recognize failure situations that require adaptations. In our experiments, we are able to show that detection accuracies above 90% can be achieved with current object detection methods.},
  doi      = {https://doi.org/10.1016/j.procs.2021.04.009},
  keywords = {Process Monitoring, Object Detection, Computer Vision, Machine Learning, Industry 4.0, Cyber-Physical Production Systems},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050921007821},
}

@Article{Kim2020,
  author  = {Kim, Jinwoo and Hwang, Jeongbin and Chi, Seokho and Seo, JoonOh},
  journal = {Automation in Construction},
  title   = {Towards database-free vision-based monitoring on construction sites: A deep active learning approach},
  year    = {2020},
  month   = {12},
  pages   = {103376},
  volume  = {120},
  doi     = {10.1016/j.autcon.2020.103376},
}

@Article{Wu2022,
  author  = {Wu, Junpeng and Tang, Shaobo and Li, Xianglei and Zhou, Yibo},
  journal = {PLOS ONE},
  title   = {Industrial equipment detection algorithm under complex working conditions based on ROMS R-CNN},
  year    = {2022},
  month   = {04},
  pages   = {e0266444},
  volume  = {17},
  doi     = {10.1371/journal.pone.0266444},
}

@Online{labelimg,
  journal = {{LabelImg}},
  note    = {\url{https://github.com/tzutalin/labelImg/}, last accessed on 2022-07-29},
  title   = {{LabelImg}: an annotation tool},
  urldate = {2022-07-29},
  year    = {2015},
}

@Comment{jabref-meta: databaseType:bibtex;}
